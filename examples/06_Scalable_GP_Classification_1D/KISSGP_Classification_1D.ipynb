{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use a `GridInducingVariationalGP` module. This classification module is designed for when the inputs of the function you're modeling are one-dimensional.\n",
    "\n",
    "The use of inducing points allows for scaling up the training data by making computational complexity linear instead of cubic.\n",
    "\n",
    "In this example, weâ€™re modeling a function that is periodically labeled cycling every 1/8 (think of a square wave with period 1/4)\n",
    "\n",
    "This notebook doesn't use cuda, in general we recommend GPU use if possible and most of our notebooks utilize cuda as well.\n",
    "\n",
    "Kernel interpolation for scalable structured Gaussian processes (KISS-GP) was introduced in this paper:\n",
    "http://proceedings.mlr.press/v37/wilson15.pdf\n",
    "\n",
    "KISS-GP with SVI for classification was introduced in this paper:\n",
    "https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# The train data points are spaced every 1/25 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 26))\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign\n",
    "# periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (2 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from math import exp\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from gpytorch.priors import SmoothedBoxPrior\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=32, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n",
    "        # RBF kernel as universal approximator\n",
    "        self.covar_module = RBFKernel(\n",
    "            log_lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1, log_transform=True)\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            name=\"log_outputscale\",\n",
    "            parameter=torch.nn.Parameter(torch.Tensor([0])),\n",
    "            prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1, log_transform=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "# Initialize model\n",
    "model = GPClassificationModel()\n",
    "# Use Bernoulli Likelihood (warps via normal CDF to (0,1))\n",
    "likelihood = BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 611.932   log_lengthscale: 0.000   log_outputscale: 0.000\n",
      "Iter 1/200 - log_lengthscale_grad: 39.535   log_outputscale_grad: -608030.938\n",
      "Iter 2/200 - Loss: 439.384   log_lengthscale: -0.100   log_outputscale: 0.100\n",
      "Iter 2/200 - log_lengthscale_grad: 38.641   log_outputscale_grad: -481624.312\n",
      "Iter 3/200 - Loss: 317.409   log_lengthscale: -0.200   log_outputscale: 0.199\n",
      "Iter 3/200 - log_lengthscale_grad: 40.319   log_outputscale_grad: -382741.406\n",
      "Iter 4/200 - Loss: 236.955   log_lengthscale: -0.300   log_outputscale: 0.295\n",
      "Iter 4/200 - log_lengthscale_grad: 48.675   log_outputscale_grad: -312305.688\n",
      "Iter 5/200 - Loss: 172.992   log_lengthscale: -0.400   log_outputscale: 0.390\n",
      "Iter 5/200 - log_lengthscale_grad: 52.310   log_outputscale_grad: -247659.672\n",
      "Iter 6/200 - Loss: 118.262   log_lengthscale: -0.501   log_outputscale: 0.481\n",
      "Iter 6/200 - log_lengthscale_grad: 48.333   log_outputscale_grad: -183569.016\n",
      "Iter 7/200 - Loss: 76.162   log_lengthscale: -0.602   log_outputscale: 0.568\n",
      "Iter 7/200 - log_lengthscale_grad: 43.007   log_outputscale_grad: -127520.672\n",
      "Iter 8/200 - Loss: 49.725   log_lengthscale: -0.703   log_outputscale: 0.650\n",
      "Iter 8/200 - log_lengthscale_grad: 38.096   log_outputscale_grad: -87956.742\n",
      "Iter 9/200 - Loss: 34.694   log_lengthscale: -0.803   log_outputscale: 0.726\n",
      "Iter 9/200 - log_lengthscale_grad: 33.161   log_outputscale_grad: -64769.930\n",
      "Iter 10/200 - Loss: 26.467   log_lengthscale: -0.901   log_outputscale: 0.797\n",
      "Iter 10/200 - log_lengthscale_grad: 29.067   log_outputscale_grad: -51345.562\n",
      "Iter 11/200 - Loss: 21.646   log_lengthscale: -0.997   log_outputscale: 0.863\n",
      "Iter 11/200 - log_lengthscale_grad: 25.092   log_outputscale_grad: -42585.457\n",
      "Iter 12/200 - Loss: 18.922   log_lengthscale: -1.091   log_outputscale: 0.924\n",
      "Iter 12/200 - log_lengthscale_grad: 22.173   log_outputscale_grad: -37446.367\n",
      "Iter 13/200 - Loss: 17.243   log_lengthscale: -1.182   log_outputscale: 0.980\n",
      "Iter 13/200 - log_lengthscale_grad: 20.105   log_outputscale_grad: -36095.984\n",
      "Iter 14/200 - Loss: 17.088   log_lengthscale: -1.270   log_outputscale: 1.033\n",
      "Iter 14/200 - log_lengthscale_grad: 16.832   log_outputscale_grad: -38497.148\n",
      "Iter 15/200 - Loss: 18.239   log_lengthscale: -1.355   log_outputscale: 1.083\n",
      "Iter 15/200 - log_lengthscale_grad: 15.273   log_outputscale_grad: -41408.984\n",
      "Iter 16/200 - Loss: 16.714   log_lengthscale: -1.436   log_outputscale: 1.129\n",
      "Iter 16/200 - log_lengthscale_grad: 14.753   log_outputscale_grad: -42252.473\n",
      "Iter 17/200 - Loss: 15.316   log_lengthscale: -1.515   log_outputscale: 1.174\n",
      "Iter 17/200 - log_lengthscale_grad: 13.185   log_outputscale_grad: -41339.875\n",
      "Iter 18/200 - Loss: 14.162   log_lengthscale: -1.591   log_outputscale: 1.216\n",
      "Iter 18/200 - log_lengthscale_grad: 12.508   log_outputscale_grad: -40119.602\n",
      "Iter 19/200 - Loss: 13.016   log_lengthscale: -1.664   log_outputscale: 1.257\n",
      "Iter 19/200 - log_lengthscale_grad: 11.197   log_outputscale_grad: -38983.367\n",
      "Iter 20/200 - Loss: 11.692   log_lengthscale: -1.734   log_outputscale: 1.296\n",
      "Iter 20/200 - log_lengthscale_grad: 10.887   log_outputscale_grad: -36935.652\n",
      "Iter 21/200 - Loss: 10.694   log_lengthscale: -1.802   log_outputscale: 1.333\n",
      "Iter 21/200 - log_lengthscale_grad: 10.051   log_outputscale_grad: -34203.117\n",
      "Iter 22/200 - Loss: 9.312   log_lengthscale: -1.867   log_outputscale: 1.368\n",
      "Iter 22/200 - log_lengthscale_grad: 9.614   log_outputscale_grad: -30223.949\n",
      "Iter 23/200 - Loss: 8.320   log_lengthscale: -1.929   log_outputscale: 1.402\n",
      "Iter 23/200 - log_lengthscale_grad: 8.104   log_outputscale_grad: -26004.359\n",
      "Iter 24/200 - Loss: 7.034   log_lengthscale: -1.989   log_outputscale: 1.434\n",
      "Iter 24/200 - log_lengthscale_grad: 7.812   log_outputscale_grad: -22060.168\n",
      "Iter 25/200 - Loss: 6.057   log_lengthscale: -2.046   log_outputscale: 1.465\n",
      "Iter 25/200 - log_lengthscale_grad: 7.227   log_outputscale_grad: -18550.502\n",
      "Iter 26/200 - Loss: 5.180   log_lengthscale: -2.101   log_outputscale: 1.494\n",
      "Iter 26/200 - log_lengthscale_grad: 6.963   log_outputscale_grad: -14979.296\n",
      "Iter 27/200 - Loss: 4.842   log_lengthscale: -2.154   log_outputscale: 1.521\n",
      "Iter 27/200 - log_lengthscale_grad: 6.489   log_outputscale_grad: -11829.096\n",
      "Iter 28/200 - Loss: 3.953   log_lengthscale: -2.204   log_outputscale: 1.546\n",
      "Iter 28/200 - log_lengthscale_grad: 6.678   log_outputscale_grad: -9224.782\n",
      "Iter 29/200 - Loss: 4.146   log_lengthscale: -2.253   log_outputscale: 1.570\n",
      "Iter 29/200 - log_lengthscale_grad: 5.828   log_outputscale_grad: -7280.745\n",
      "Iter 30/200 - Loss: 3.536   log_lengthscale: -2.299   log_outputscale: 1.592\n",
      "Iter 30/200 - log_lengthscale_grad: 5.404   log_outputscale_grad: -5434.737\n",
      "Iter 31/200 - Loss: 3.259   log_lengthscale: -2.343   log_outputscale: 1.612\n",
      "Iter 31/200 - log_lengthscale_grad: 4.924   log_outputscale_grad: -3471.160\n",
      "Iter 32/200 - Loss: 2.699   log_lengthscale: -2.386   log_outputscale: 1.631\n",
      "Iter 32/200 - log_lengthscale_grad: 5.325   log_outputscale_grad: -1785.760\n",
      "Iter 33/200 - Loss: 2.478   log_lengthscale: -2.426   log_outputscale: 1.648\n",
      "Iter 33/200 - log_lengthscale_grad: 2.912   log_outputscale_grad: -1354.617\n",
      "Iter 34/200 - Loss: 2.634   log_lengthscale: -2.465   log_outputscale: 1.664\n",
      "Iter 34/200 - log_lengthscale_grad: 2.311   log_outputscale_grad: -653.981\n",
      "Iter 35/200 - Loss: 2.416   log_lengthscale: -2.501   log_outputscale: 1.678\n",
      "Iter 35/200 - log_lengthscale_grad: 2.251   log_outputscale_grad: -235.094\n",
      "Iter 36/200 - Loss: 3.144   log_lengthscale: -2.534   log_outputscale: 1.691\n",
      "Iter 36/200 - log_lengthscale_grad: 1.707   log_outputscale_grad: -121.509\n",
      "Iter 37/200 - Loss: 2.721   log_lengthscale: -2.566   log_outputscale: 1.703\n",
      "Iter 37/200 - log_lengthscale_grad: 0.503   log_outputscale_grad: 37.295\n",
      "Iter 38/200 - Loss: 3.057   log_lengthscale: -2.594   log_outputscale: 1.714\n",
      "Iter 38/200 - log_lengthscale_grad: 0.243   log_outputscale_grad: 323.259\n",
      "Iter 39/200 - Loss: 3.188   log_lengthscale: -2.621   log_outputscale: 1.724\n",
      "Iter 39/200 - log_lengthscale_grad: -0.572   log_outputscale_grad: 92.696\n",
      "Iter 40/200 - Loss: 2.661   log_lengthscale: -2.644   log_outputscale: 1.733\n",
      "Iter 40/200 - log_lengthscale_grad: -1.046   log_outputscale_grad: 350.939\n",
      "Iter 41/200 - Loss: 3.215   log_lengthscale: -2.665   log_outputscale: 1.741\n",
      "Iter 41/200 - log_lengthscale_grad: -1.257   log_outputscale_grad: 242.283\n",
      "Iter 42/200 - Loss: 3.236   log_lengthscale: -2.684   log_outputscale: 1.748\n",
      "Iter 42/200 - log_lengthscale_grad: -1.888   log_outputscale_grad: 260.172\n",
      "Iter 43/200 - Loss: 2.751   log_lengthscale: -2.700   log_outputscale: 1.755\n",
      "Iter 43/200 - log_lengthscale_grad: -1.899   log_outputscale_grad: 236.302\n",
      "Iter 44/200 - Loss: 2.956   log_lengthscale: -2.714   log_outputscale: 1.761\n",
      "Iter 44/200 - log_lengthscale_grad: -2.377   log_outputscale_grad: 339.767\n",
      "Iter 45/200 - Loss: 3.100   log_lengthscale: -2.725   log_outputscale: 1.766\n",
      "Iter 45/200 - log_lengthscale_grad: -1.334   log_outputscale_grad: 307.815\n",
      "Iter 46/200 - Loss: 2.589   log_lengthscale: -2.735   log_outputscale: 1.771\n",
      "Iter 46/200 - log_lengthscale_grad: -2.061   log_outputscale_grad: 402.264\n",
      "Iter 47/200 - Loss: 2.905   log_lengthscale: -2.743   log_outputscale: 1.776\n",
      "Iter 47/200 - log_lengthscale_grad: -2.081   log_outputscale_grad: 316.887\n",
      "Iter 48/200 - Loss: 3.388   log_lengthscale: -2.749   log_outputscale: 1.780\n",
      "Iter 48/200 - log_lengthscale_grad: -2.847   log_outputscale_grad: 245.288\n",
      "Iter 49/200 - Loss: 2.931   log_lengthscale: -2.753   log_outputscale: 1.784\n",
      "Iter 49/200 - log_lengthscale_grad: -2.061   log_outputscale_grad: 358.260\n",
      "Iter 50/200 - Loss: 2.873   log_lengthscale: -2.756   log_outputscale: 1.787\n",
      "Iter 50/200 - log_lengthscale_grad: -2.502   log_outputscale_grad: 297.772\n",
      "Iter 51/200 - Loss: 3.039   log_lengthscale: -2.757   log_outputscale: 1.790\n",
      "Iter 51/200 - log_lengthscale_grad: -2.324   log_outputscale_grad: 299.172\n",
      "Iter 52/200 - Loss: 2.774   log_lengthscale: -2.757   log_outputscale: 1.793\n",
      "Iter 52/200 - log_lengthscale_grad: -2.308   log_outputscale_grad: 286.872\n",
      "Iter 53/200 - Loss: 2.749   log_lengthscale: -2.756   log_outputscale: 1.795\n",
      "Iter 53/200 - log_lengthscale_grad: -2.702   log_outputscale_grad: 230.529\n",
      "Iter 54/200 - Loss: 2.800   log_lengthscale: -2.754   log_outputscale: 1.797\n",
      "Iter 54/200 - log_lengthscale_grad: -2.728   log_outputscale_grad: 262.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 55/200 - Loss: 2.485   log_lengthscale: -2.750   log_outputscale: 1.799\n",
      "Iter 55/200 - log_lengthscale_grad: -1.758   log_outputscale_grad: 372.178\n",
      "Iter 56/200 - Loss: 2.334   log_lengthscale: -2.746   log_outputscale: 1.801\n",
      "Iter 56/200 - log_lengthscale_grad: -1.777   log_outputscale_grad: 490.321\n",
      "Iter 57/200 - Loss: 2.578   log_lengthscale: -2.741   log_outputscale: 1.803\n",
      "Iter 57/200 - log_lengthscale_grad: -2.277   log_outputscale_grad: 389.020\n",
      "Iter 58/200 - Loss: 2.578   log_lengthscale: -2.736   log_outputscale: 1.804\n",
      "Iter 58/200 - log_lengthscale_grad: -1.549   log_outputscale_grad: 385.131\n",
      "Iter 59/200 - Loss: 2.332   log_lengthscale: -2.730   log_outputscale: 1.805\n",
      "Iter 59/200 - log_lengthscale_grad: -2.005   log_outputscale_grad: 473.167\n",
      "Iter 60/200 - Loss: 2.517   log_lengthscale: -2.723   log_outputscale: 1.806\n",
      "Iter 60/200 - log_lengthscale_grad: -1.233   log_outputscale_grad: 402.149\n",
      "Iter 61/200 - Loss: 2.062   log_lengthscale: -2.717   log_outputscale: 1.807\n",
      "Iter 61/200 - log_lengthscale_grad: -2.885   log_outputscale_grad: 347.892\n",
      "Iter 62/200 - Loss: 2.666   log_lengthscale: -2.709   log_outputscale: 1.808\n",
      "Iter 62/200 - log_lengthscale_grad: -2.144   log_outputscale_grad: 395.678\n",
      "Iter 63/200 - Loss: 2.572   log_lengthscale: -2.702   log_outputscale: 1.809\n",
      "Iter 63/200 - log_lengthscale_grad: -1.807   log_outputscale_grad: 319.382\n",
      "Iter 64/200 - Loss: 2.500   log_lengthscale: -2.693   log_outputscale: 1.809\n",
      "Iter 64/200 - log_lengthscale_grad: -2.018   log_outputscale_grad: 409.786\n",
      "Iter 65/200 - Loss: 2.692   log_lengthscale: -2.685   log_outputscale: 1.810\n",
      "Iter 65/200 - log_lengthscale_grad: -1.897   log_outputscale_grad: 450.461\n",
      "Iter 66/200 - Loss: 2.444   log_lengthscale: -2.676   log_outputscale: 1.810\n",
      "Iter 66/200 - log_lengthscale_grad: -0.396   log_outputscale_grad: 573.715\n",
      "Iter 67/200 - Loss: 1.954   log_lengthscale: -2.668   log_outputscale: 1.811\n",
      "Iter 67/200 - log_lengthscale_grad: -1.351   log_outputscale_grad: 540.666\n",
      "Iter 68/200 - Loss: 2.189   log_lengthscale: -2.659   log_outputscale: 1.811\n",
      "Iter 68/200 - log_lengthscale_grad: -1.119   log_outputscale_grad: 420.575\n",
      "Iter 69/200 - Loss: 1.957   log_lengthscale: -2.651   log_outputscale: 1.811\n",
      "Iter 69/200 - log_lengthscale_grad: -0.601   log_outputscale_grad: 468.942\n",
      "Iter 70/200 - Loss: 2.363   log_lengthscale: -2.644   log_outputscale: 1.811\n",
      "Iter 70/200 - log_lengthscale_grad: -0.675   log_outputscale_grad: 265.961\n",
      "Iter 71/200 - Loss: 2.116   log_lengthscale: -2.636   log_outputscale: 1.812\n",
      "Iter 71/200 - log_lengthscale_grad: -0.594   log_outputscale_grad: 342.858\n",
      "Iter 72/200 - Loss: 2.056   log_lengthscale: -2.629   log_outputscale: 1.812\n",
      "Iter 72/200 - log_lengthscale_grad: 0.021   log_outputscale_grad: 305.530\n",
      "Iter 73/200 - Loss: 2.261   log_lengthscale: -2.623   log_outputscale: 1.812\n",
      "Iter 73/200 - log_lengthscale_grad: 0.269   log_outputscale_grad: 196.069\n",
      "Iter 74/200 - Loss: 1.924   log_lengthscale: -2.617   log_outputscale: 1.812\n",
      "Iter 74/200 - log_lengthscale_grad: -1.276   log_outputscale_grad: 492.217\n",
      "Iter 75/200 - Loss: 1.929   log_lengthscale: -2.611   log_outputscale: 1.812\n",
      "Iter 75/200 - log_lengthscale_grad: -1.408   log_outputscale_grad: 511.054\n",
      "Iter 76/200 - Loss: 1.804   log_lengthscale: -2.605   log_outputscale: 1.812\n",
      "Iter 76/200 - log_lengthscale_grad: -0.578   log_outputscale_grad: 377.885\n",
      "Iter 77/200 - Loss: 2.280   log_lengthscale: -2.599   log_outputscale: 1.812\n",
      "Iter 77/200 - log_lengthscale_grad: -0.534   log_outputscale_grad: 516.312\n",
      "Iter 78/200 - Loss: 1.881   log_lengthscale: -2.593   log_outputscale: 1.811\n",
      "Iter 78/200 - log_lengthscale_grad: -1.250   log_outputscale_grad: 431.582\n",
      "Iter 79/200 - Loss: 1.818   log_lengthscale: -2.588   log_outputscale: 1.811\n",
      "Iter 79/200 - log_lengthscale_grad: -1.326   log_outputscale_grad: 433.328\n",
      "Iter 80/200 - Loss: 1.923   log_lengthscale: -2.581   log_outputscale: 1.811\n",
      "Iter 80/200 - log_lengthscale_grad: -0.917   log_outputscale_grad: 513.076\n",
      "Iter 81/200 - Loss: 1.589   log_lengthscale: -2.575   log_outputscale: 1.811\n",
      "Iter 81/200 - log_lengthscale_grad: -0.029   log_outputscale_grad: 479.673\n",
      "Iter 82/200 - Loss: 2.694   log_lengthscale: -2.569   log_outputscale: 1.811\n",
      "Iter 82/200 - log_lengthscale_grad: -0.656   log_outputscale_grad: 274.169\n",
      "Iter 83/200 - Loss: 2.020   log_lengthscale: -2.564   log_outputscale: 1.810\n",
      "Iter 83/200 - log_lengthscale_grad: -1.097   log_outputscale_grad: 303.413\n",
      "Iter 84/200 - Loss: 2.305   log_lengthscale: -2.558   log_outputscale: 1.810\n",
      "Iter 84/200 - log_lengthscale_grad: 0.194   log_outputscale_grad: 401.735\n",
      "Iter 85/200 - Loss: 1.926   log_lengthscale: -2.553   log_outputscale: 1.810\n",
      "Iter 85/200 - log_lengthscale_grad: -0.666   log_outputscale_grad: 484.480\n",
      "Iter 86/200 - Loss: 1.630   log_lengthscale: -2.548   log_outputscale: 1.810\n",
      "Iter 86/200 - log_lengthscale_grad: 0.002   log_outputscale_grad: 411.613\n",
      "Iter 87/200 - Loss: 1.914   log_lengthscale: -2.544   log_outputscale: 1.809\n",
      "Iter 87/200 - log_lengthscale_grad: -0.938   log_outputscale_grad: 356.673\n",
      "Iter 88/200 - Loss: 1.935   log_lengthscale: -2.539   log_outputscale: 1.809\n",
      "Iter 88/200 - log_lengthscale_grad: 1.511   log_outputscale_grad: 199.979\n",
      "Iter 89/200 - Loss: 1.488   log_lengthscale: -2.536   log_outputscale: 1.809\n",
      "Iter 89/200 - log_lengthscale_grad: 1.954   log_outputscale_grad: 333.855\n",
      "Iter 90/200 - Loss: 1.904   log_lengthscale: -2.534   log_outputscale: 1.809\n",
      "Iter 90/200 - log_lengthscale_grad: 0.159   log_outputscale_grad: 337.518\n",
      "Iter 91/200 - Loss: 2.040   log_lengthscale: -2.533   log_outputscale: 1.808\n",
      "Iter 91/200 - log_lengthscale_grad: 1.191   log_outputscale_grad: 175.326\n",
      "Iter 92/200 - Loss: 1.808   log_lengthscale: -2.532   log_outputscale: 1.808\n",
      "Iter 92/200 - log_lengthscale_grad: -0.328   log_outputscale_grad: 145.265\n",
      "Iter 93/200 - Loss: 1.554   log_lengthscale: -2.531   log_outputscale: 1.808\n",
      "Iter 93/200 - log_lengthscale_grad: 1.393   log_outputscale_grad: 269.180\n",
      "Iter 94/200 - Loss: 2.098   log_lengthscale: -2.532   log_outputscale: 1.807\n",
      "Iter 94/200 - log_lengthscale_grad: 1.381   log_outputscale_grad: -56.213\n",
      "Iter 95/200 - Loss: 1.431   log_lengthscale: -2.533   log_outputscale: 1.807\n",
      "Iter 95/200 - log_lengthscale_grad: 0.591   log_outputscale_grad: 73.410\n",
      "Iter 96/200 - Loss: 1.671   log_lengthscale: -2.535   log_outputscale: 1.807\n",
      "Iter 96/200 - log_lengthscale_grad: 1.928   log_outputscale_grad: 91.641\n",
      "Iter 97/200 - Loss: 1.916   log_lengthscale: -2.537   log_outputscale: 1.807\n",
      "Iter 97/200 - log_lengthscale_grad: 1.194   log_outputscale_grad: 65.908\n",
      "Iter 98/200 - Loss: 1.285   log_lengthscale: -2.541   log_outputscale: 1.806\n",
      "Iter 98/200 - log_lengthscale_grad: 1.118   log_outputscale_grad: 453.105\n",
      "Iter 99/200 - Loss: 1.570   log_lengthscale: -2.545   log_outputscale: 1.806\n",
      "Iter 99/200 - log_lengthscale_grad: 0.102   log_outputscale_grad: 544.440\n",
      "Iter 100/200 - Loss: 1.551   log_lengthscale: -2.548   log_outputscale: 1.806\n",
      "Iter 100/200 - log_lengthscale_grad: 1.951   log_outputscale_grad: 283.648\n",
      "Iter 101/200 - Loss: 1.839   log_lengthscale: -2.553   log_outputscale: 1.806\n",
      "Iter 101/200 - log_lengthscale_grad: 1.231   log_outputscale_grad: 70.170\n",
      "Iter 102/200 - Loss: 1.567   log_lengthscale: -2.557   log_outputscale: 1.805\n",
      "Iter 102/200 - log_lengthscale_grad: 0.765   log_outputscale_grad: 414.858\n",
      "Iter 103/200 - Loss: 1.828   log_lengthscale: -2.563   log_outputscale: 1.805\n",
      "Iter 103/200 - log_lengthscale_grad: -0.209   log_outputscale_grad: 199.723\n",
      "Iter 104/200 - Loss: 1.611   log_lengthscale: -2.567   log_outputscale: 1.805\n",
      "Iter 104/200 - log_lengthscale_grad: -0.055   log_outputscale_grad: 327.506\n",
      "Iter 105/200 - Loss: 1.632   log_lengthscale: -2.571   log_outputscale: 1.805\n",
      "Iter 105/200 - log_lengthscale_grad: 0.382   log_outputscale_grad: 293.671\n",
      "Iter 106/200 - Loss: 1.350   log_lengthscale: -2.575   log_outputscale: 1.804\n",
      "Iter 106/200 - log_lengthscale_grad: -0.841   log_outputscale_grad: 361.995\n",
      "Iter 107/200 - Loss: 1.951   log_lengthscale: -2.578   log_outputscale: 1.804\n",
      "Iter 107/200 - log_lengthscale_grad: -0.594   log_outputscale_grad: 106.824\n",
      "Iter 108/200 - Loss: 1.925   log_lengthscale: -2.580   log_outputscale: 1.804\n",
      "Iter 108/200 - log_lengthscale_grad: -0.401   log_outputscale_grad: 325.099\n",
      "Iter 109/200 - Loss: 1.277   log_lengthscale: -2.581   log_outputscale: 1.804\n",
      "Iter 109/200 - log_lengthscale_grad: 0.388   log_outputscale_grad: 395.655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 110/200 - Loss: 1.679   log_lengthscale: -2.583   log_outputscale: 1.803\n",
      "Iter 110/200 - log_lengthscale_grad: 0.415   log_outputscale_grad: 319.616\n",
      "Iter 111/200 - Loss: 1.708   log_lengthscale: -2.585   log_outputscale: 1.803\n",
      "Iter 111/200 - log_lengthscale_grad: -0.042   log_outputscale_grad: 197.004\n",
      "Iter 112/200 - Loss: 1.243   log_lengthscale: -2.587   log_outputscale: 1.803\n",
      "Iter 112/200 - log_lengthscale_grad: -0.214   log_outputscale_grad: 378.070\n",
      "Iter 113/200 - Loss: 1.512   log_lengthscale: -2.588   log_outputscale: 1.802\n",
      "Iter 113/200 - log_lengthscale_grad: -0.555   log_outputscale_grad: 345.824\n",
      "Iter 114/200 - Loss: 1.359   log_lengthscale: -2.589   log_outputscale: 1.802\n",
      "Iter 114/200 - log_lengthscale_grad: -0.128   log_outputscale_grad: 505.112\n",
      "Iter 115/200 - Loss: 1.439   log_lengthscale: -2.590   log_outputscale: 1.802\n",
      "Iter 115/200 - log_lengthscale_grad: 0.513   log_outputscale_grad: 438.800\n",
      "Iter 116/200 - Loss: 1.618   log_lengthscale: -2.591   log_outputscale: 1.801\n",
      "Iter 116/200 - log_lengthscale_grad: -1.058   log_outputscale_grad: 323.503\n",
      "Iter 117/200 - Loss: 1.525   log_lengthscale: -2.591   log_outputscale: 1.801\n",
      "Iter 117/200 - log_lengthscale_grad: -0.277   log_outputscale_grad: 434.933\n",
      "Iter 118/200 - Loss: 1.255   log_lengthscale: -2.591   log_outputscale: 1.801\n",
      "Iter 118/200 - log_lengthscale_grad: 0.072   log_outputscale_grad: 410.257\n",
      "Iter 119/200 - Loss: 1.487   log_lengthscale: -2.591   log_outputscale: 1.800\n",
      "Iter 119/200 - log_lengthscale_grad: 0.273   log_outputscale_grad: 322.380\n",
      "Iter 120/200 - Loss: 1.333   log_lengthscale: -2.591   log_outputscale: 1.800\n",
      "Iter 120/200 - log_lengthscale_grad: -0.320   log_outputscale_grad: 493.810\n",
      "Iter 121/200 - Loss: 1.392   log_lengthscale: -2.591   log_outputscale: 1.799\n",
      "Iter 121/200 - log_lengthscale_grad: 0.178   log_outputscale_grad: 385.264\n",
      "Iter 122/200 - Loss: 1.240   log_lengthscale: -2.591   log_outputscale: 1.799\n",
      "Iter 122/200 - log_lengthscale_grad: 0.911   log_outputscale_grad: 448.307\n",
      "Iter 123/200 - Loss: 1.350   log_lengthscale: -2.591   log_outputscale: 1.799\n",
      "Iter 123/200 - log_lengthscale_grad: -0.395   log_outputscale_grad: 240.224\n",
      "Iter 124/200 - Loss: 1.637   log_lengthscale: -2.592   log_outputscale: 1.798\n",
      "Iter 124/200 - log_lengthscale_grad: 0.787   log_outputscale_grad: 368.077\n",
      "Iter 125/200 - Loss: 1.452   log_lengthscale: -2.593   log_outputscale: 1.798\n",
      "Iter 125/200 - log_lengthscale_grad: -0.187   log_outputscale_grad: 265.060\n",
      "Iter 126/200 - Loss: 1.400   log_lengthscale: -2.593   log_outputscale: 1.797\n",
      "Iter 126/200 - log_lengthscale_grad: -0.222   log_outputscale_grad: 258.881\n",
      "Iter 127/200 - Loss: 1.551   log_lengthscale: -2.594   log_outputscale: 1.797\n",
      "Iter 127/200 - log_lengthscale_grad: -1.144   log_outputscale_grad: 273.677\n",
      "Iter 128/200 - Loss: 0.913   log_lengthscale: -2.593   log_outputscale: 1.797\n",
      "Iter 128/200 - log_lengthscale_grad: -1.238   log_outputscale_grad: 354.770\n",
      "Iter 129/200 - Loss: 1.477   log_lengthscale: -2.592   log_outputscale: 1.796\n",
      "Iter 129/200 - log_lengthscale_grad: 0.369   log_outputscale_grad: 269.110\n",
      "Iter 130/200 - Loss: 1.172   log_lengthscale: -2.591   log_outputscale: 1.796\n",
      "Iter 130/200 - log_lengthscale_grad: 0.372   log_outputscale_grad: 299.849\n",
      "Iter 131/200 - Loss: 1.307   log_lengthscale: -2.590   log_outputscale: 1.795\n",
      "Iter 131/200 - log_lengthscale_grad: -0.906   log_outputscale_grad: 445.743\n",
      "Iter 132/200 - Loss: 1.282   log_lengthscale: -2.589   log_outputscale: 1.795\n",
      "Iter 132/200 - log_lengthscale_grad: -0.721   log_outputscale_grad: 405.410\n",
      "Iter 133/200 - Loss: 1.199   log_lengthscale: -2.587   log_outputscale: 1.795\n",
      "Iter 133/200 - log_lengthscale_grad: -0.288   log_outputscale_grad: 403.917\n",
      "Iter 134/200 - Loss: 1.525   log_lengthscale: -2.586   log_outputscale: 1.794\n",
      "Iter 134/200 - log_lengthscale_grad: 0.908   log_outputscale_grad: 321.168\n",
      "Iter 135/200 - Loss: 1.251   log_lengthscale: -2.585   log_outputscale: 1.794\n",
      "Iter 135/200 - log_lengthscale_grad: -0.705   log_outputscale_grad: 338.912\n",
      "Iter 136/200 - Loss: 1.250   log_lengthscale: -2.583   log_outputscale: 1.793\n",
      "Iter 136/200 - log_lengthscale_grad: 0.790   log_outputscale_grad: 537.736\n",
      "Iter 137/200 - Loss: 1.443   log_lengthscale: -2.583   log_outputscale: 1.793\n",
      "Iter 137/200 - log_lengthscale_grad: -0.619   log_outputscale_grad: 388.208\n",
      "Iter 138/200 - Loss: 1.307   log_lengthscale: -2.582   log_outputscale: 1.792\n",
      "Iter 138/200 - log_lengthscale_grad: 0.387   log_outputscale_grad: 392.636\n",
      "Iter 139/200 - Loss: 1.641   log_lengthscale: -2.581   log_outputscale: 1.792\n",
      "Iter 139/200 - log_lengthscale_grad: 0.170   log_outputscale_grad: 267.638\n",
      "Iter 140/200 - Loss: 1.517   log_lengthscale: -2.580   log_outputscale: 1.792\n",
      "Iter 140/200 - log_lengthscale_grad: 0.031   log_outputscale_grad: 208.167\n",
      "Iter 141/200 - Loss: 1.232   log_lengthscale: -2.580   log_outputscale: 1.791\n",
      "Iter 141/200 - log_lengthscale_grad: -0.442   log_outputscale_grad: 226.964\n",
      "Iter 142/200 - Loss: 1.488   log_lengthscale: -2.579   log_outputscale: 1.791\n",
      "Iter 142/200 - log_lengthscale_grad: 0.560   log_outputscale_grad: 422.494\n",
      "Iter 143/200 - Loss: 1.078   log_lengthscale: -2.579   log_outputscale: 1.790\n",
      "Iter 143/200 - log_lengthscale_grad: -0.560   log_outputscale_grad: 363.867\n",
      "Iter 144/200 - Loss: 1.373   log_lengthscale: -2.579   log_outputscale: 1.790\n",
      "Iter 144/200 - log_lengthscale_grad: 0.024   log_outputscale_grad: 208.512\n",
      "Iter 145/200 - Loss: 1.670   log_lengthscale: -2.578   log_outputscale: 1.789\n",
      "Iter 145/200 - log_lengthscale_grad: 0.978   log_outputscale_grad: 188.557\n",
      "Iter 146/200 - Loss: 0.916   log_lengthscale: -2.578   log_outputscale: 1.789\n",
      "Iter 146/200 - log_lengthscale_grad: 0.243   log_outputscale_grad: 437.708\n",
      "Iter 147/200 - Loss: 1.494   log_lengthscale: -2.579   log_outputscale: 1.789\n",
      "Iter 147/200 - log_lengthscale_grad: 0.430   log_outputscale_grad: 415.327\n",
      "Iter 148/200 - Loss: 1.020   log_lengthscale: -2.580   log_outputscale: 1.788\n",
      "Iter 148/200 - log_lengthscale_grad: 0.271   log_outputscale_grad: 364.888\n",
      "Iter 149/200 - Loss: 1.541   log_lengthscale: -2.581   log_outputscale: 1.788\n",
      "Iter 149/200 - log_lengthscale_grad: 0.350   log_outputscale_grad: 258.661\n",
      "Iter 150/200 - Loss: 1.299   log_lengthscale: -2.582   log_outputscale: 1.787\n",
      "Iter 150/200 - log_lengthscale_grad: 0.276   log_outputscale_grad: 173.333\n",
      "Iter 151/200 - Loss: 1.473   log_lengthscale: -2.583   log_outputscale: 1.787\n",
      "Iter 151/200 - log_lengthscale_grad: -0.048   log_outputscale_grad: 262.127\n",
      "Iter 152/200 - Loss: 0.928   log_lengthscale: -2.585   log_outputscale: 1.787\n",
      "Iter 152/200 - log_lengthscale_grad: 1.046   log_outputscale_grad: 502.943\n",
      "Iter 153/200 - Loss: 1.507   log_lengthscale: -2.587   log_outputscale: 1.786\n",
      "Iter 153/200 - log_lengthscale_grad: 0.827   log_outputscale_grad: 164.499\n",
      "Iter 154/200 - Loss: 1.509   log_lengthscale: -2.589   log_outputscale: 1.786\n",
      "Iter 154/200 - log_lengthscale_grad: 0.830   log_outputscale_grad: 190.622\n",
      "Iter 155/200 - Loss: 1.177   log_lengthscale: -2.592   log_outputscale: 1.785\n",
      "Iter 155/200 - log_lengthscale_grad: 0.650   log_outputscale_grad: 282.346\n",
      "Iter 156/200 - Loss: 1.446   log_lengthscale: -2.595   log_outputscale: 1.785\n",
      "Iter 156/200 - log_lengthscale_grad: 0.035   log_outputscale_grad: 168.201\n",
      "Iter 157/200 - Loss: 1.075   log_lengthscale: -2.598   log_outputscale: 1.785\n",
      "Iter 157/200 - log_lengthscale_grad: 1.138   log_outputscale_grad: 249.989\n",
      "Iter 158/200 - Loss: 1.788   log_lengthscale: -2.602   log_outputscale: 1.784\n",
      "Iter 158/200 - log_lengthscale_grad: -0.089   log_outputscale_grad: 12.188\n",
      "Iter 159/200 - Loss: 1.233   log_lengthscale: -2.605   log_outputscale: 1.784\n",
      "Iter 159/200 - log_lengthscale_grad: -0.213   log_outputscale_grad: 161.010\n",
      "Iter 160/200 - Loss: 1.453   log_lengthscale: -2.608   log_outputscale: 1.784\n",
      "Iter 160/200 - log_lengthscale_grad: 1.288   log_outputscale_grad: 163.581\n",
      "Iter 161/200 - Loss: 1.447   log_lengthscale: -2.612   log_outputscale: 1.783\n",
      "Iter 161/200 - log_lengthscale_grad: 0.096   log_outputscale_grad: 214.264\n",
      "Iter 162/200 - Loss: 1.621   log_lengthscale: -2.615   log_outputscale: 1.783\n",
      "Iter 162/200 - log_lengthscale_grad: 0.168   log_outputscale_grad: 250.149\n",
      "Iter 163/200 - Loss: 1.217   log_lengthscale: -2.618   log_outputscale: 1.783\n",
      "Iter 163/200 - log_lengthscale_grad: 0.043   log_outputscale_grad: 237.282\n",
      "Iter 164/200 - Loss: 1.234   log_lengthscale: -2.621   log_outputscale: 1.782\n",
      "Iter 164/200 - log_lengthscale_grad: 0.643   log_outputscale_grad: 335.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 165/200 - Loss: 1.457   log_lengthscale: -2.625   log_outputscale: 1.782\n",
      "Iter 165/200 - log_lengthscale_grad: -1.184   log_outputscale_grad: 221.114\n",
      "Iter 166/200 - Loss: 1.305   log_lengthscale: -2.627   log_outputscale: 1.782\n",
      "Iter 166/200 - log_lengthscale_grad: -0.339   log_outputscale_grad: 491.702\n",
      "Iter 167/200 - Loss: 1.029   log_lengthscale: -2.628   log_outputscale: 1.781\n",
      "Iter 167/200 - log_lengthscale_grad: -0.439   log_outputscale_grad: 408.740\n",
      "Iter 168/200 - Loss: 1.114   log_lengthscale: -2.629   log_outputscale: 1.781\n",
      "Iter 168/200 - log_lengthscale_grad: -0.560   log_outputscale_grad: 487.961\n",
      "Iter 169/200 - Loss: 1.222   log_lengthscale: -2.629   log_outputscale: 1.781\n",
      "Iter 169/200 - log_lengthscale_grad: -0.226   log_outputscale_grad: 253.755\n",
      "Iter 170/200 - Loss: 1.007   log_lengthscale: -2.629   log_outputscale: 1.780\n",
      "Iter 170/200 - log_lengthscale_grad: -0.572   log_outputscale_grad: 225.861\n",
      "Iter 171/200 - Loss: 1.291   log_lengthscale: -2.628   log_outputscale: 1.780\n",
      "Iter 171/200 - log_lengthscale_grad: -0.633   log_outputscale_grad: 304.374\n",
      "Iter 172/200 - Loss: 1.207   log_lengthscale: -2.627   log_outputscale: 1.779\n",
      "Iter 172/200 - log_lengthscale_grad: -0.652   log_outputscale_grad: 341.183\n",
      "Iter 173/200 - Loss: 1.776   log_lengthscale: -2.626   log_outputscale: 1.779\n",
      "Iter 173/200 - log_lengthscale_grad: -0.440   log_outputscale_grad: 257.263\n",
      "Iter 174/200 - Loss: 1.111   log_lengthscale: -2.624   log_outputscale: 1.778\n",
      "Iter 174/200 - log_lengthscale_grad: -0.823   log_outputscale_grad: 282.411\n",
      "Iter 175/200 - Loss: 1.380   log_lengthscale: -2.621   log_outputscale: 1.778\n",
      "Iter 175/200 - log_lengthscale_grad: 0.576   log_outputscale_grad: 329.893\n",
      "Iter 176/200 - Loss: 1.002   log_lengthscale: -2.620   log_outputscale: 1.778\n",
      "Iter 176/200 - log_lengthscale_grad: -1.287   log_outputscale_grad: 393.196\n",
      "Iter 177/200 - Loss: 1.122   log_lengthscale: -2.617   log_outputscale: 1.777\n",
      "Iter 177/200 - log_lengthscale_grad: -0.047   log_outputscale_grad: 358.895\n",
      "Iter 178/200 - Loss: 1.362   log_lengthscale: -2.614   log_outputscale: 1.777\n",
      "Iter 178/200 - log_lengthscale_grad: -0.889   log_outputscale_grad: 319.021\n",
      "Iter 179/200 - Loss: 1.184   log_lengthscale: -2.611   log_outputscale: 1.776\n",
      "Iter 179/200 - log_lengthscale_grad: -0.053   log_outputscale_grad: 305.037\n",
      "Iter 180/200 - Loss: 1.570   log_lengthscale: -2.608   log_outputscale: 1.776\n",
      "Iter 180/200 - log_lengthscale_grad: 0.161   log_outputscale_grad: 309.574\n",
      "Iter 181/200 - Loss: 1.460   log_lengthscale: -2.606   log_outputscale: 1.776\n",
      "Iter 181/200 - log_lengthscale_grad: 0.871   log_outputscale_grad: 290.978\n",
      "Iter 182/200 - Loss: 1.253   log_lengthscale: -2.605   log_outputscale: 1.775\n",
      "Iter 182/200 - log_lengthscale_grad: 0.430   log_outputscale_grad: 422.780\n",
      "Iter 183/200 - Loss: 1.198   log_lengthscale: -2.604   log_outputscale: 1.775\n",
      "Iter 183/200 - log_lengthscale_grad: 0.279   log_outputscale_grad: 172.545\n",
      "Iter 184/200 - Loss: 0.970   log_lengthscale: -2.603   log_outputscale: 1.774\n",
      "Iter 184/200 - log_lengthscale_grad: 0.343   log_outputscale_grad: 337.091\n",
      "Iter 185/200 - Loss: 1.177   log_lengthscale: -2.603   log_outputscale: 1.774\n",
      "Iter 185/200 - log_lengthscale_grad: 0.231   log_outputscale_grad: 221.058\n",
      "Iter 186/200 - Loss: 1.178   log_lengthscale: -2.604   log_outputscale: 1.773\n",
      "Iter 186/200 - log_lengthscale_grad: 1.718   log_outputscale_grad: 323.909\n",
      "Iter 187/200 - Loss: 1.351   log_lengthscale: -2.605   log_outputscale: 1.773\n",
      "Iter 187/200 - log_lengthscale_grad: -0.528   log_outputscale_grad: 166.263\n",
      "Iter 188/200 - Loss: 1.378   log_lengthscale: -2.606   log_outputscale: 1.772\n",
      "Iter 188/200 - log_lengthscale_grad: 0.387   log_outputscale_grad: -33.269\n",
      "Iter 189/200 - Loss: 1.324   log_lengthscale: -2.608   log_outputscale: 1.772\n",
      "Iter 189/200 - log_lengthscale_grad: 0.706   log_outputscale_grad: 247.464\n",
      "Iter 190/200 - Loss: 1.603   log_lengthscale: -2.610   log_outputscale: 1.772\n",
      "Iter 190/200 - log_lengthscale_grad: 0.465   log_outputscale_grad: -48.342\n",
      "Iter 191/200 - Loss: 1.324   log_lengthscale: -2.612   log_outputscale: 1.771\n",
      "Iter 191/200 - log_lengthscale_grad: 0.190   log_outputscale_grad: 101.312\n",
      "Iter 192/200 - Loss: 1.378   log_lengthscale: -2.614   log_outputscale: 1.771\n",
      "Iter 192/200 - log_lengthscale_grad: 0.623   log_outputscale_grad: 288.760\n",
      "Iter 193/200 - Loss: 1.214   log_lengthscale: -2.617   log_outputscale: 1.771\n",
      "Iter 193/200 - log_lengthscale_grad: 0.277   log_outputscale_grad: 84.409\n",
      "Iter 194/200 - Loss: 1.294   log_lengthscale: -2.620   log_outputscale: 1.770\n",
      "Iter 194/200 - log_lengthscale_grad: 0.641   log_outputscale_grad: 402.050\n",
      "Iter 195/200 - Loss: 1.275   log_lengthscale: -2.623   log_outputscale: 1.770\n",
      "Iter 195/200 - log_lengthscale_grad: 0.503   log_outputscale_grad: 177.744\n",
      "Iter 196/200 - Loss: 1.457   log_lengthscale: -2.626   log_outputscale: 1.770\n",
      "Iter 196/200 - log_lengthscale_grad: 0.360   log_outputscale_grad: 187.665\n",
      "Iter 197/200 - Loss: 1.400   log_lengthscale: -2.629   log_outputscale: 1.769\n",
      "Iter 197/200 - log_lengthscale_grad: 0.826   log_outputscale_grad: 130.714\n",
      "Iter 198/200 - Loss: 1.067   log_lengthscale: -2.633   log_outputscale: 1.769\n",
      "Iter 198/200 - log_lengthscale_grad: -0.065   log_outputscale_grad: 185.545\n",
      "Iter 199/200 - Loss: 1.280   log_lengthscale: -2.636   log_outputscale: 1.769\n",
      "Iter 199/200 - log_lengthscale_grad: 0.446   log_outputscale_grad: 269.410\n",
      "Iter 200/200 - Loss: 1.400   log_lengthscale: -2.640   log_outputscale: 1.769\n",
      "Iter 200/200 - log_lengthscale_grad: -0.171   log_outputscale_grad: 181.594\n",
      "CPU times: user 52.8 s, sys: 719 ms, total: 53.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    # BernoulliLikelihood has no parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# n_data refers to the amount of training data\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=len(train_y))\n",
    "\n",
    "def train():\n",
    "    num_iter = 200\n",
    "    for i in range(num_iter):\n",
    "        # Zero gradients out for new iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calculate loss\n",
    "        loss = -mll(output, train_y)\n",
    "        # Calc gradients\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f   log_outputscale: %.3f' % (\n",
    "            i + 1, num_iter, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "            model.log_outputscale.data.item(),\n",
    "        ))\n",
    "        \n",
    "        print('Iter %d/%d - log_lengthscale_grad: %.3f   log_outputscale_grad: %.3f' % (\n",
    "            i + 1, num_iter,\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.grad.item(),\n",
    "            model.log_outputscale.grad.item(),\n",
    "        ))\n",
    "        optimizer.step()\n",
    "        \n",
    "# Get clock time\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADSCAYAAACo7W6xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMRhjPCwxGGM8/j8j/RxzoDzXlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfbc123ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set model and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize axes\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are every 0.01 from 0 to 1 inclusive \n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "# Make predictions from model output Gaussian warped through Bernoulli likelihood\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Black stars for trainng data\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    # Based of prediction probability label -1 or 1\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    # Plot test predictions as blue line\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "# Call plot\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
