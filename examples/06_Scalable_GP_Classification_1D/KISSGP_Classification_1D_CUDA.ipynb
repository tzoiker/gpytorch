{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use a `GridInducingVariationalGP` module. This classification module is designed for when the inputs of the function you're modeling are one-dimensional.\n",
    "\n",
    "The use of inducing points allows for scaling up the training data by making computational complexity linear instead of cubic.\n",
    "\n",
    "In this example, weâ€™re modeling a function that is periodically labeled cycling every 1/8 (think of a square wave with period 1/4)\n",
    "\n",
    "Kernel interpolation for scalable structured Gaussian processes (KISS-GP) was introduced in this paper:\n",
    "http://proceedings.mlr.press/v37/wilson15.pdf\n",
    "\n",
    "KISS-GP with SVI for classification was introduced in this paper:\n",
    "https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# the train data points are spaced every 1/499 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 500)).cuda()\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi)))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel, GridInterpolationKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood, BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel, self).__init__(grid_size=100, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-5, 5))\n",
    "        # The priors over the hyperparameters probably aren't necessary, but let's use them anyways\n",
    "        self.covar_module = RBFKernel(\n",
    "            log_lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1, log_transform=True)\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            name=\"log_outputscale\",\n",
    "            parameter=torch.nn.Parameter(torch.Tensor([0])),\n",
    "            prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1, log_transform=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "# Initialize model\n",
    "model = GPClassificationModel().cuda()\n",
    "# Use Bernoulli Likelihood (warps via normal CDF to (0,1))\n",
    "likelihood = BernoulliLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 466.424   log_lengthscale: 0.000\n",
      "Iter 2/200 - Loss: 465.799   log_lengthscale: -0.100\n",
      "Iter 3/200 - Loss: 290.081   log_lengthscale: -0.177\n",
      "Iter 4/200 - Loss: 247.155   log_lengthscale: -0.254\n",
      "Iter 5/200 - Loss: 189.488   log_lengthscale: -0.331\n",
      "Iter 6/200 - Loss: 125.033   log_lengthscale: -0.405\n",
      "Iter 7/200 - Loss: 72.036   log_lengthscale: -0.476\n",
      "Iter 8/200 - Loss: 54.739   log_lengthscale: -0.543\n",
      "Iter 9/200 - Loss: 37.632   log_lengthscale: -0.606\n",
      "Iter 10/200 - Loss: 31.019   log_lengthscale: -0.666\n",
      "Iter 11/200 - Loss: 21.672   log_lengthscale: -0.726\n",
      "Iter 12/200 - Loss: 14.138   log_lengthscale: -0.784\n",
      "Iter 13/200 - Loss: 7.934   log_lengthscale: -0.840\n",
      "Iter 14/200 - Loss: 14.829   log_lengthscale: -0.895\n",
      "Iter 15/200 - Loss: 19.902   log_lengthscale: -0.944\n",
      "Iter 16/200 - Loss: 22.650   log_lengthscale: -0.989\n",
      "Iter 17/200 - Loss: 23.586   log_lengthscale: -1.033\n",
      "Iter 18/200 - Loss: 26.247   log_lengthscale: -1.076\n",
      "Iter 19/200 - Loss: 25.541   log_lengthscale: -1.118\n",
      "Iter 20/200 - Loss: 23.347   log_lengthscale: -1.161\n",
      "Iter 21/200 - Loss: 22.599   log_lengthscale: -1.206\n",
      "Iter 22/200 - Loss: 17.147   log_lengthscale: -1.250\n",
      "Iter 23/200 - Loss: 12.173   log_lengthscale: -1.296\n",
      "Iter 24/200 - Loss: 11.278   log_lengthscale: -1.345\n",
      "Iter 25/200 - Loss: 7.928   log_lengthscale: -1.395\n",
      "Iter 26/200 - Loss: 6.835   log_lengthscale: -1.444\n",
      "Iter 27/200 - Loss: 5.261   log_lengthscale: -1.492\n",
      "Iter 28/200 - Loss: 5.912   log_lengthscale: -1.542\n",
      "Iter 29/200 - Loss: 4.155   log_lengthscale: -1.592\n",
      "Iter 30/200 - Loss: 2.527   log_lengthscale: -1.637\n",
      "Iter 31/200 - Loss: 3.807   log_lengthscale: -1.682\n",
      "Iter 32/200 - Loss: 2.174   log_lengthscale: -1.731\n",
      "Iter 33/200 - Loss: 2.184   log_lengthscale: -1.778\n",
      "Iter 34/200 - Loss: 2.410   log_lengthscale: -1.824\n",
      "Iter 35/200 - Loss: 2.040   log_lengthscale: -1.866\n",
      "Iter 36/200 - Loss: 1.740   log_lengthscale: -1.905\n",
      "Iter 37/200 - Loss: 1.551   log_lengthscale: -1.941\n",
      "Iter 38/200 - Loss: 1.404   log_lengthscale: -1.978\n",
      "Iter 39/200 - Loss: 1.556   log_lengthscale: -2.013\n",
      "Iter 40/200 - Loss: 1.518   log_lengthscale: -2.047\n",
      "Iter 41/200 - Loss: 1.124   log_lengthscale: -2.079\n",
      "Iter 42/200 - Loss: 0.842   log_lengthscale: -2.108\n",
      "Iter 43/200 - Loss: 1.681   log_lengthscale: -2.136\n",
      "Iter 44/200 - Loss: 1.090   log_lengthscale: -2.164\n",
      "Iter 45/200 - Loss: 1.180   log_lengthscale: -2.191\n",
      "Iter 46/200 - Loss: 1.610   log_lengthscale: -2.217\n",
      "Iter 47/200 - Loss: 1.485   log_lengthscale: -2.241\n",
      "Iter 48/200 - Loss: 1.195   log_lengthscale: -2.264\n",
      "Iter 49/200 - Loss: 1.087   log_lengthscale: -2.283\n",
      "Iter 50/200 - Loss: 1.127   log_lengthscale: -2.299\n",
      "Iter 51/200 - Loss: 1.408   log_lengthscale: -2.315\n",
      "Iter 52/200 - Loss: 1.085   log_lengthscale: -2.331\n",
      "Iter 53/200 - Loss: 1.563   log_lengthscale: -2.347\n",
      "Iter 54/200 - Loss: 1.247   log_lengthscale: -2.361\n",
      "Iter 55/200 - Loss: 1.196   log_lengthscale: -2.376\n",
      "Iter 56/200 - Loss: 0.999   log_lengthscale: -2.392\n",
      "Iter 57/200 - Loss: 1.196   log_lengthscale: -2.409\n",
      "Iter 58/200 - Loss: 1.040   log_lengthscale: -2.426\n",
      "Iter 59/200 - Loss: 0.914   log_lengthscale: -2.444\n",
      "Iter 60/200 - Loss: 1.013   log_lengthscale: -2.462\n",
      "Iter 61/200 - Loss: 1.130   log_lengthscale: -2.479\n",
      "Iter 62/200 - Loss: 1.002   log_lengthscale: -2.493\n",
      "Iter 63/200 - Loss: 1.167   log_lengthscale: -2.506\n",
      "Iter 64/200 - Loss: 0.935   log_lengthscale: -2.519\n",
      "Iter 65/200 - Loss: 0.710   log_lengthscale: -2.532\n",
      "Iter 66/200 - Loss: 0.989   log_lengthscale: -2.545\n",
      "Iter 67/200 - Loss: 1.003   log_lengthscale: -2.557\n",
      "Iter 68/200 - Loss: 1.009   log_lengthscale: -2.566\n",
      "Iter 69/200 - Loss: 0.957   log_lengthscale: -2.575\n",
      "Iter 70/200 - Loss: 0.933   log_lengthscale: -2.579\n",
      "Iter 71/200 - Loss: 1.055   log_lengthscale: -2.585\n",
      "Iter 72/200 - Loss: 0.966   log_lengthscale: -2.591\n",
      "Iter 73/200 - Loss: 0.920   log_lengthscale: -2.597\n",
      "Iter 74/200 - Loss: 0.861   log_lengthscale: -2.601\n",
      "Iter 75/200 - Loss: 0.992   log_lengthscale: -2.607\n",
      "Iter 76/200 - Loss: 1.191   log_lengthscale: -2.610\n",
      "Iter 77/200 - Loss: 0.667   log_lengthscale: -2.612\n",
      "Iter 78/200 - Loss: 0.797   log_lengthscale: -2.613\n",
      "Iter 79/200 - Loss: 1.266   log_lengthscale: -2.616\n",
      "Iter 80/200 - Loss: 1.003   log_lengthscale: -2.618\n",
      "Iter 81/200 - Loss: 1.034   log_lengthscale: -2.618\n",
      "Iter 82/200 - Loss: 0.913   log_lengthscale: -2.620\n",
      "Iter 83/200 - Loss: 0.720   log_lengthscale: -2.622\n",
      "Iter 84/200 - Loss: 1.006   log_lengthscale: -2.628\n",
      "Iter 85/200 - Loss: 0.865   log_lengthscale: -2.633\n",
      "Iter 86/200 - Loss: 0.973   log_lengthscale: -2.639\n",
      "Iter 87/200 - Loss: 0.812   log_lengthscale: -2.644\n",
      "Iter 88/200 - Loss: 0.849   log_lengthscale: -2.648\n",
      "Iter 89/200 - Loss: 0.958   log_lengthscale: -2.651\n",
      "Iter 90/200 - Loss: 0.853   log_lengthscale: -2.654\n",
      "Iter 91/200 - Loss: 0.891   log_lengthscale: -2.659\n",
      "Iter 92/200 - Loss: 1.171   log_lengthscale: -2.663\n",
      "Iter 93/200 - Loss: 1.048   log_lengthscale: -2.666\n",
      "Iter 94/200 - Loss: 0.925   log_lengthscale: -2.670\n",
      "Iter 95/200 - Loss: 1.039   log_lengthscale: -2.674\n",
      "Iter 96/200 - Loss: 0.815   log_lengthscale: -2.679\n",
      "Iter 97/200 - Loss: 0.840   log_lengthscale: -2.682\n",
      "Iter 98/200 - Loss: 0.932   log_lengthscale: -2.685\n",
      "Iter 99/200 - Loss: 0.934   log_lengthscale: -2.688\n",
      "Iter 100/200 - Loss: 0.980   log_lengthscale: -2.690\n",
      "Iter 101/200 - Loss: 0.883   log_lengthscale: -2.694\n",
      "Iter 102/200 - Loss: 1.083   log_lengthscale: -2.698\n",
      "Iter 103/200 - Loss: 0.980   log_lengthscale: -2.702\n",
      "Iter 104/200 - Loss: 0.729   log_lengthscale: -2.707\n",
      "Iter 105/200 - Loss: 0.714   log_lengthscale: -2.714\n",
      "Iter 106/200 - Loss: 0.774   log_lengthscale: -2.719\n",
      "Iter 107/200 - Loss: 0.970   log_lengthscale: -2.726\n",
      "Iter 108/200 - Loss: 0.890   log_lengthscale: -2.732\n",
      "Iter 109/200 - Loss: 0.845   log_lengthscale: -2.739\n",
      "Iter 110/200 - Loss: 0.953   log_lengthscale: -2.745\n",
      "Iter 111/200 - Loss: 1.000   log_lengthscale: -2.750\n",
      "Iter 112/200 - Loss: 0.948   log_lengthscale: -2.756\n",
      "Iter 113/200 - Loss: 1.016   log_lengthscale: -2.763\n",
      "Iter 114/200 - Loss: 0.828   log_lengthscale: -2.768\n",
      "Iter 115/200 - Loss: 0.865   log_lengthscale: -2.774\n",
      "Iter 116/200 - Loss: 0.748   log_lengthscale: -2.778\n",
      "Iter 117/200 - Loss: 1.107   log_lengthscale: -2.781\n",
      "Iter 118/200 - Loss: 0.963   log_lengthscale: -2.784\n",
      "Iter 119/200 - Loss: 1.248   log_lengthscale: -2.790\n",
      "Iter 120/200 - Loss: 0.995   log_lengthscale: -2.795\n",
      "Iter 121/200 - Loss: 0.670   log_lengthscale: -2.799\n",
      "Iter 122/200 - Loss: 1.024   log_lengthscale: -2.804\n",
      "Iter 123/200 - Loss: 0.815   log_lengthscale: -2.811\n",
      "Iter 124/200 - Loss: 0.959   log_lengthscale: -2.816\n",
      "Iter 125/200 - Loss: 1.362   log_lengthscale: -2.820\n",
      "Iter 126/200 - Loss: 1.445   log_lengthscale: -2.827\n",
      "Iter 127/200 - Loss: 0.952   log_lengthscale: -2.835\n",
      "Iter 128/200 - Loss: 0.799   log_lengthscale: -2.843\n",
      "Iter 129/200 - Loss: 0.732   log_lengthscale: -2.851\n",
      "Iter 130/200 - Loss: 0.774   log_lengthscale: -2.857\n",
      "Iter 131/200 - Loss: 1.071   log_lengthscale: -2.864\n",
      "Iter 132/200 - Loss: 0.801   log_lengthscale: -2.871\n",
      "Iter 133/200 - Loss: 1.233   log_lengthscale: -2.880\n",
      "Iter 134/200 - Loss: 1.188   log_lengthscale: -2.887\n",
      "Iter 135/200 - Loss: 0.791   log_lengthscale: -2.891\n",
      "Iter 136/200 - Loss: 0.702   log_lengthscale: -2.891\n",
      "Iter 137/200 - Loss: 0.719   log_lengthscale: -2.890\n",
      "Iter 138/200 - Loss: 0.845   log_lengthscale: -2.890\n",
      "Iter 139/200 - Loss: 0.812   log_lengthscale: -2.889\n",
      "Iter 140/200 - Loss: 0.931   log_lengthscale: -2.887\n",
      "Iter 141/200 - Loss: 1.271   log_lengthscale: -2.886\n",
      "Iter 142/200 - Loss: 1.196   log_lengthscale: -2.885\n",
      "Iter 143/200 - Loss: 1.593   log_lengthscale: -2.884\n",
      "Iter 144/200 - Loss: 1.006   log_lengthscale: -2.884\n",
      "Iter 145/200 - Loss: 0.670   log_lengthscale: -2.885\n",
      "Iter 146/200 - Loss: 0.626   log_lengthscale: -2.886\n",
      "Iter 147/200 - Loss: 0.753   log_lengthscale: -2.886\n",
      "Iter 148/200 - Loss: 0.898   log_lengthscale: -2.889\n",
      "Iter 149/200 - Loss: 1.083   log_lengthscale: -2.890\n",
      "Iter 150/200 - Loss: 1.073   log_lengthscale: -2.892\n",
      "Iter 151/200 - Loss: 0.821   log_lengthscale: -2.895\n",
      "Iter 152/200 - Loss: 0.606   log_lengthscale: -2.899\n",
      "Iter 153/200 - Loss: 0.911   log_lengthscale: -2.901\n",
      "Iter 154/200 - Loss: 0.845   log_lengthscale: -2.903\n",
      "Iter 155/200 - Loss: 0.797   log_lengthscale: -2.902\n",
      "Iter 156/200 - Loss: 0.790   log_lengthscale: -2.900\n",
      "Iter 157/200 - Loss: 0.934   log_lengthscale: -2.902\n",
      "Iter 158/200 - Loss: 1.731   log_lengthscale: -2.901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 159/200 - Loss: 0.970   log_lengthscale: -2.901\n",
      "Iter 160/200 - Loss: 0.699   log_lengthscale: -2.901\n",
      "Iter 161/200 - Loss: 0.626   log_lengthscale: -2.900\n",
      "Iter 162/200 - Loss: 0.708   log_lengthscale: -2.899\n",
      "Iter 163/200 - Loss: 0.851   log_lengthscale: -2.898\n",
      "Iter 164/200 - Loss: 0.837   log_lengthscale: -2.898\n",
      "Iter 165/200 - Loss: 1.032   log_lengthscale: -2.900\n",
      "Iter 166/200 - Loss: 0.864   log_lengthscale: -2.906\n",
      "Iter 167/200 - Loss: 1.097   log_lengthscale: -2.911\n",
      "Iter 168/200 - Loss: 1.039   log_lengthscale: -2.916\n",
      "Iter 169/200 - Loss: 0.811   log_lengthscale: -2.920\n",
      "Iter 170/200 - Loss: 0.736   log_lengthscale: -2.924\n",
      "Iter 171/200 - Loss: 0.663   log_lengthscale: -2.929\n",
      "Iter 172/200 - Loss: 0.948   log_lengthscale: -2.936\n",
      "Iter 173/200 - Loss: 0.656   log_lengthscale: -2.944\n",
      "Iter 174/200 - Loss: 0.628   log_lengthscale: -2.954\n",
      "Iter 175/200 - Loss: 0.695   log_lengthscale: -2.964\n",
      "Iter 176/200 - Loss: 0.881   log_lengthscale: -2.974\n",
      "Iter 177/200 - Loss: 0.858   log_lengthscale: -2.986\n",
      "Iter 178/200 - Loss: 0.767   log_lengthscale: -2.998\n",
      "Iter 179/200 - Loss: 0.808   log_lengthscale: -3.012\n",
      "Iter 180/200 - Loss: 0.953   log_lengthscale: -3.025\n",
      "Iter 181/200 - Loss: 0.693   log_lengthscale: -3.040\n",
      "Iter 182/200 - Loss: 0.559   log_lengthscale: -3.052\n",
      "Iter 183/200 - Loss: 0.664   log_lengthscale: -3.063\n",
      "Iter 184/200 - Loss: 0.582   log_lengthscale: -3.073\n",
      "Iter 185/200 - Loss: 0.783   log_lengthscale: -3.084\n",
      "Iter 186/200 - Loss: 0.645   log_lengthscale: -3.095\n",
      "Iter 187/200 - Loss: 0.849   log_lengthscale: -3.104\n",
      "Iter 188/200 - Loss: 0.548   log_lengthscale: -3.112\n",
      "Iter 189/200 - Loss: 0.603   log_lengthscale: -3.119\n",
      "Iter 190/200 - Loss: 0.671   log_lengthscale: -3.127\n",
      "Iter 191/200 - Loss: 0.653   log_lengthscale: -3.134\n",
      "Iter 192/200 - Loss: 0.631   log_lengthscale: -3.141\n",
      "Iter 193/200 - Loss: 0.653   log_lengthscale: -3.148\n",
      "Iter 194/200 - Loss: 0.698   log_lengthscale: -3.155\n",
      "Iter 195/200 - Loss: 0.653   log_lengthscale: -3.162\n",
      "Iter 196/200 - Loss: 0.628   log_lengthscale: -3.170\n",
      "Iter 197/200 - Loss: 0.571   log_lengthscale: -3.176\n",
      "Iter 198/200 - Loss: 0.603   log_lengthscale: -3.183\n",
      "Iter 199/200 - Loss: 0.658   log_lengthscale: -3.190\n",
      "Iter 200/200 - Loss: 0.604   log_lengthscale: -3.197\n",
      "CPU times: user 6.66 s, sys: 108 ms, total: 6.76 s\n",
      "Wall time: 6.75 s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    # BernoulliLikelihood has no parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# n_data refers to the amount of training data\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=len(train_y))\n",
    "\n",
    "def train():\n",
    "    # Use adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    optimizer.n_iter = 0\n",
    "    total_iter = 200\n",
    "    for i in range(total_iter):\n",
    "        # Zero gradients out for new iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calculate loss\n",
    "        loss = -mll(output, train_y)\n",
    "        # Calc gradients\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "            i + 1, total_iter, loss.data[0],\n",
    "            model.covar_module.base_kernel_module.log_lengthscale.data.squeeze()[0],\n",
    "        ))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of use_toeplitz\n",
    "with gpytorch.settings.use_toeplitz(False):\n",
    "    # Get time spent\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f797e6cfeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put mopdel and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize axes\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are every 0.01 from 0 to 1 inclusive \n",
    "test_x = Variable(torch.linspace(0, 1, 1001)).cuda()\n",
    "# Make predictions from model output Gaussian warped through Bernoulli likelihood\n",
    "predictions = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Black stars for trainng data\n",
    "    ax.plot(train_x.data.cpu().numpy(), train_y.data.cpu().numpy(), 'k*')\n",
    "    # Based of prediction probability label -1 or 1\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    # Plot test predictions as blue line\n",
    "    ax.plot(test_x.data.cpu().numpy(), pred_labels.data.cpu().numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Call plot\n",
    "ax_plot(observed_ax, predictions, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
