{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Stochastic Variational GP Regression in 1D (w/ KISS-GP)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This example shows how to perform GP regression, but using **variational inference** rather than exact inference. There are a few cases where variational inference may be prefereable:\n",
    "\n",
    "1) If you have lots of data, and want to perform **stochastic optimization**\n",
    "\n",
    "2) If you have a model where you want to use other variational distributions\n",
    "\n",
    "KISS-GP with SVI was introduced in:\n",
    "https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf\n",
    "\n",
    "**NOTE: Variational inference in GPyTorch is still in its early stages - and the interface is likely to change in the near future!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set\n",
    "# We're going to learn a sine function\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "train_y = torch.sin(train_x * (4 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SGD - the dataloader\n",
    "\n",
    "Because we want to do stochastic optimization, we have to put the dataset in a pytorch **DataLoader**.\n",
    "This creates easy minibatches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "This is pretty similar to a normal regression model, except now we're using a `gpytorch.models.GridInducingVariationalGP` instead of a `gpytorch.models.ExactGP`.\n",
    "\n",
    "Any of the variational models would work. We're using the `GridInducingVariationalGP` because we have many data points, but only 1 dimensional data.\n",
    "\n",
    "Similar to exact regression, we use a `GaussianLikelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPRegressionModel, self).__init__(grid_size=20, grid_bounds=[(-0.05, 1.05)])\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                log_lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-3), math.exp(6), sigma=0.1, log_transform=True\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "model = GPRegressionModel()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "This training loop will use **stochastic optimization** rather than batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/40 - Loss: 16.507\n",
      "Iter 1/40 - Loss: 14.814\n",
      "Iter 1/40 - Loss: 13.362\n",
      "Iter 1/40 - Loss: 12.253\n",
      "Iter 1/40 - Loss: 11.350\n",
      "Iter 1/40 - Loss: 10.589\n",
      "Iter 1/40 - Loss: 10.041\n",
      "Iter 1/40 - Loss: 9.489\n",
      "Iter 1/40 - Loss: 9.229\n",
      "Iter 1/40 - Loss: 8.890\n",
      "Iter 1/40 - Loss: 8.636\n",
      "Iter 1/40 - Loss: 8.443\n",
      "Iter 1/40 - Loss: 8.226\n",
      "Iter 1/40 - Loss: 8.060\n",
      "Iter 1/40 - Loss: 7.951\n",
      "Iter 1/40 - Loss: 7.818\n",
      "Iter 2/40 - Loss: 7.903\n",
      "Iter 2/40 - Loss: 7.655\n",
      "Iter 2/40 - Loss: 7.674\n",
      "Iter 2/40 - Loss: 7.692\n",
      "Iter 2/40 - Loss: 7.594\n",
      "Iter 2/40 - Loss: 7.442\n",
      "Iter 2/40 - Loss: 7.513\n",
      "Iter 2/40 - Loss: 7.542\n",
      "Iter 2/40 - Loss: 7.500\n",
      "Iter 2/40 - Loss: 7.446\n",
      "Iter 2/40 - Loss: 7.397\n",
      "Iter 2/40 - Loss: 7.441\n",
      "Iter 2/40 - Loss: 7.400\n",
      "Iter 2/40 - Loss: 7.396\n",
      "Iter 2/40 - Loss: 7.320\n",
      "Iter 2/40 - Loss: 7.338\n",
      "Iter 3/40 - Loss: 7.260\n",
      "Iter 3/40 - Loss: 7.354\n",
      "Iter 3/40 - Loss: 7.250\n",
      "Iter 3/40 - Loss: 7.378\n",
      "Iter 3/40 - Loss: 7.325\n",
      "Iter 3/40 - Loss: 7.363\n",
      "Iter 3/40 - Loss: 7.334\n",
      "Iter 3/40 - Loss: 7.314\n",
      "Iter 3/40 - Loss: 7.261\n",
      "Iter 3/40 - Loss: 7.260\n",
      "Iter 3/40 - Loss: 7.335\n",
      "Iter 3/40 - Loss: 7.247\n",
      "Iter 3/40 - Loss: 7.327\n",
      "Iter 3/40 - Loss: 7.281\n",
      "Iter 3/40 - Loss: 7.216\n",
      "Iter 3/40 - Loss: 7.287\n",
      "Iter 4/40 - Loss: 7.178\n",
      "Iter 4/40 - Loss: 7.273\n",
      "Iter 4/40 - Loss: 7.180\n",
      "Iter 4/40 - Loss: 7.154\n",
      "Iter 4/40 - Loss: 7.199\n",
      "Iter 4/40 - Loss: 7.297\n",
      "Iter 4/40 - Loss: 7.098\n",
      "Iter 4/40 - Loss: 7.295\n",
      "Iter 4/40 - Loss: 7.207\n",
      "Iter 4/40 - Loss: 7.260\n",
      "Iter 4/40 - Loss: 7.136\n",
      "Iter 4/40 - Loss: 7.119\n",
      "Iter 4/40 - Loss: 7.147\n",
      "Iter 4/40 - Loss: 7.215\n",
      "Iter 4/40 - Loss: 7.108\n",
      "Iter 4/40 - Loss: 7.142\n",
      "Iter 5/40 - Loss: 7.146\n",
      "Iter 5/40 - Loss: 7.192\n",
      "Iter 5/40 - Loss: 7.143\n",
      "Iter 5/40 - Loss: 7.108\n",
      "Iter 5/40 - Loss: 7.014\n",
      "Iter 5/40 - Loss: 7.022\n",
      "Iter 5/40 - Loss: 7.078\n",
      "Iter 5/40 - Loss: 7.130\n",
      "Iter 5/40 - Loss: 7.011\n",
      "Iter 5/40 - Loss: 7.026\n",
      "Iter 5/40 - Loss: 7.065\n",
      "Iter 5/40 - Loss: 7.033\n",
      "Iter 5/40 - Loss: 7.036\n",
      "Iter 5/40 - Loss: 7.026\n",
      "Iter 5/40 - Loss: 7.089\n",
      "Iter 5/40 - Loss: 7.017\n",
      "Iter 6/40 - Loss: 7.071\n",
      "Iter 6/40 - Loss: 6.983\n",
      "Iter 6/40 - Loss: 7.024\n",
      "Iter 6/40 - Loss: 6.899\n",
      "Iter 6/40 - Loss: 6.925\n",
      "Iter 6/40 - Loss: 6.989\n",
      "Iter 6/40 - Loss: 6.896\n",
      "Iter 6/40 - Loss: 6.873\n",
      "Iter 6/40 - Loss: 6.933\n",
      "Iter 6/40 - Loss: 6.829\n",
      "Iter 6/40 - Loss: 6.972\n",
      "Iter 6/40 - Loss: 6.935\n",
      "Iter 6/40 - Loss: 6.922\n",
      "Iter 6/40 - Loss: 6.907\n",
      "Iter 6/40 - Loss: 6.935\n",
      "Iter 6/40 - Loss: 6.815\n",
      "Iter 7/40 - Loss: 6.826\n",
      "Iter 7/40 - Loss: 6.820\n",
      "Iter 7/40 - Loss: 6.828\n",
      "Iter 7/40 - Loss: 6.781\n",
      "Iter 7/40 - Loss: 6.828\n",
      "Iter 7/40 - Loss: 6.748\n",
      "Iter 7/40 - Loss: 6.904\n",
      "Iter 7/40 - Loss: 6.859\n",
      "Iter 7/40 - Loss: 6.776\n",
      "Iter 7/40 - Loss: 6.732\n",
      "Iter 7/40 - Loss: 6.697\n",
      "Iter 7/40 - Loss: 6.690\n",
      "Iter 7/40 - Loss: 6.664\n",
      "Iter 7/40 - Loss: 6.692\n",
      "Iter 7/40 - Loss: 6.732\n",
      "Iter 7/40 - Loss: 6.728\n",
      "Iter 8/40 - Loss: 6.639\n",
      "Iter 8/40 - Loss: 6.659\n",
      "Iter 8/40 - Loss: 6.661\n",
      "Iter 8/40 - Loss: 6.626\n",
      "Iter 8/40 - Loss: 6.588\n",
      "Iter 8/40 - Loss: 6.545\n",
      "Iter 8/40 - Loss: 6.625\n",
      "Iter 8/40 - Loss: 6.622\n",
      "Iter 8/40 - Loss: 6.676\n",
      "Iter 8/40 - Loss: 6.501\n",
      "Iter 8/40 - Loss: 6.522\n",
      "Iter 8/40 - Loss: 6.578\n",
      "Iter 8/40 - Loss: 6.480\n",
      "Iter 8/40 - Loss: 6.485\n",
      "Iter 8/40 - Loss: 6.452\n",
      "Iter 8/40 - Loss: 6.501\n",
      "Iter 9/40 - Loss: 6.556\n",
      "Iter 9/40 - Loss: 6.449\n",
      "Iter 9/40 - Loss: 6.399\n",
      "Iter 9/40 - Loss: 6.394\n",
      "Iter 9/40 - Loss: 6.395\n",
      "Iter 9/40 - Loss: 6.390\n",
      "Iter 9/40 - Loss: 6.374\n",
      "Iter 9/40 - Loss: 6.351\n",
      "Iter 9/40 - Loss: 6.334\n",
      "Iter 9/40 - Loss: 6.288\n",
      "Iter 9/40 - Loss: 6.351\n",
      "Iter 9/40 - Loss: 6.287\n",
      "Iter 9/40 - Loss: 6.263\n",
      "Iter 9/40 - Loss: 6.247\n",
      "Iter 9/40 - Loss: 6.279\n",
      "Iter 9/40 - Loss: 6.307\n",
      "Iter 10/40 - Loss: 6.224\n",
      "Iter 10/40 - Loss: 6.181\n",
      "Iter 10/40 - Loss: 6.171\n",
      "Iter 10/40 - Loss: 6.222\n",
      "Iter 10/40 - Loss: 6.237\n",
      "Iter 10/40 - Loss: 6.221\n",
      "Iter 10/40 - Loss: 6.177\n",
      "Iter 10/40 - Loss: 6.155\n",
      "Iter 10/40 - Loss: 6.087\n",
      "Iter 10/40 - Loss: 6.091\n",
      "Iter 10/40 - Loss: 6.065\n",
      "Iter 10/40 - Loss: 6.159\n",
      "Iter 10/40 - Loss: 6.060\n",
      "Iter 10/40 - Loss: 6.038\n",
      "Iter 10/40 - Loss: 6.058\n",
      "Iter 10/40 - Loss: 6.093\n",
      "Iter 11/40 - Loss: 6.074\n",
      "Iter 11/40 - Loss: 6.045\n",
      "Iter 11/40 - Loss: 6.008\n",
      "Iter 11/40 - Loss: 5.996\n",
      "Iter 11/40 - Loss: 6.047\n",
      "Iter 11/40 - Loss: 6.017\n",
      "Iter 11/40 - Loss: 5.944\n",
      "Iter 11/40 - Loss: 5.937\n",
      "Iter 11/40 - Loss: 6.005\n",
      "Iter 11/40 - Loss: 5.931\n",
      "Iter 11/40 - Loss: 5.945\n",
      "Iter 11/40 - Loss: 6.035\n",
      "Iter 11/40 - Loss: 6.011\n",
      "Iter 11/40 - Loss: 5.944\n",
      "Iter 11/40 - Loss: 5.884\n",
      "Iter 11/40 - Loss: 5.891\n",
      "Iter 12/40 - Loss: 5.947\n",
      "Iter 12/40 - Loss: 5.888\n",
      "Iter 12/40 - Loss: 5.935\n",
      "Iter 12/40 - Loss: 5.882\n",
      "Iter 12/40 - Loss: 5.920\n",
      "Iter 12/40 - Loss: 5.975\n",
      "Iter 12/40 - Loss: 5.927\n",
      "Iter 12/40 - Loss: 5.920\n",
      "Iter 12/40 - Loss: 5.838\n",
      "Iter 12/40 - Loss: 5.893\n",
      "Iter 12/40 - Loss: 5.914\n",
      "Iter 12/40 - Loss: 5.872\n",
      "Iter 12/40 - Loss: 5.919\n",
      "Iter 12/40 - Loss: 5.968\n",
      "Iter 12/40 - Loss: 5.847\n",
      "Iter 12/40 - Loss: 5.936\n",
      "Iter 13/40 - Loss: 5.917\n",
      "Iter 13/40 - Loss: 5.866\n",
      "Iter 13/40 - Loss: 5.851\n",
      "Iter 13/40 - Loss: 5.877\n",
      "Iter 13/40 - Loss: 5.824\n",
      "Iter 13/40 - Loss: 5.755\n",
      "Iter 13/40 - Loss: 5.923\n",
      "Iter 13/40 - Loss: 5.898\n",
      "Iter 13/40 - Loss: 5.961\n",
      "Iter 13/40 - Loss: 5.967\n",
      "Iter 13/40 - Loss: 5.834\n",
      "Iter 13/40 - Loss: 6.011\n",
      "Iter 13/40 - Loss: 5.922\n",
      "Iter 13/40 - Loss: 5.814\n",
      "Iter 13/40 - Loss: 5.815\n",
      "Iter 13/40 - Loss: 5.879\n",
      "Iter 14/40 - Loss: 5.922\n",
      "Iter 14/40 - Loss: 5.844\n",
      "Iter 14/40 - Loss: 5.862\n",
      "Iter 14/40 - Loss: 5.823\n",
      "Iter 14/40 - Loss: 5.962\n",
      "Iter 14/40 - Loss: 5.801\n",
      "Iter 14/40 - Loss: 5.926\n",
      "Iter 14/40 - Loss: 5.746\n",
      "Iter 14/40 - Loss: 5.875\n",
      "Iter 14/40 - Loss: 5.938\n",
      "Iter 14/40 - Loss: 5.902\n",
      "Iter 14/40 - Loss: 5.875\n",
      "Iter 14/40 - Loss: 5.892\n",
      "Iter 14/40 - Loss: 5.716\n",
      "Iter 14/40 - Loss: 5.963\n",
      "Iter 14/40 - Loss: 5.990\n",
      "Iter 15/40 - Loss: 5.854\n",
      "Iter 15/40 - Loss: 5.777\n",
      "Iter 15/40 - Loss: 5.787\n",
      "Iter 15/40 - Loss: 5.930\n",
      "Iter 15/40 - Loss: 5.977\n",
      "Iter 15/40 - Loss: 5.958\n",
      "Iter 15/40 - Loss: 5.901\n",
      "Iter 15/40 - Loss: 5.884\n",
      "Iter 15/40 - Loss: 5.903\n",
      "Iter 15/40 - Loss: 5.807\n",
      "Iter 15/40 - Loss: 5.839\n",
      "Iter 15/40 - Loss: 5.858\n",
      "Iter 15/40 - Loss: 5.761\n",
      "Iter 15/40 - Loss: 6.067\n",
      "Iter 15/40 - Loss: 5.902\n",
      "Iter 15/40 - Loss: 5.741\n",
      "Iter 16/40 - Loss: 5.932\n",
      "Iter 16/40 - Loss: 5.715\n",
      "Iter 16/40 - Loss: 5.907\n",
      "Iter 16/40 - Loss: 5.862\n",
      "Iter 16/40 - Loss: 5.954\n",
      "Iter 16/40 - Loss: 5.787\n",
      "Iter 16/40 - Loss: 5.895\n",
      "Iter 16/40 - Loss: 5.832\n",
      "Iter 16/40 - Loss: 5.866\n",
      "Iter 16/40 - Loss: 5.849\n",
      "Iter 16/40 - Loss: 6.011\n",
      "Iter 16/40 - Loss: 5.930\n",
      "Iter 16/40 - Loss: 5.798\n",
      "Iter 16/40 - Loss: 5.873\n",
      "Iter 16/40 - Loss: 5.800\n",
      "Iter 16/40 - Loss: 5.964\n",
      "Iter 17/40 - Loss: 5.822\n",
      "Iter 17/40 - Loss: 5.920\n",
      "Iter 17/40 - Loss: 5.873\n",
      "Iter 17/40 - Loss: 5.898\n",
      "Iter 17/40 - Loss: 5.811\n",
      "Iter 17/40 - Loss: 5.843\n",
      "Iter 17/40 - Loss: 6.017\n",
      "Iter 17/40 - Loss: 5.938\n",
      "Iter 17/40 - Loss: 5.930\n",
      "Iter 17/40 - Loss: 5.909\n",
      "Iter 17/40 - Loss: 5.743\n",
      "Iter 17/40 - Loss: 5.825\n",
      "Iter 17/40 - Loss: 5.867\n",
      "Iter 17/40 - Loss: 5.847\n",
      "Iter 17/40 - Loss: 5.831\n",
      "Iter 17/40 - Loss: 5.856\n",
      "Iter 18/40 - Loss: 5.787\n",
      "Iter 18/40 - Loss: 6.100\n",
      "Iter 18/40 - Loss: 5.873\n",
      "Iter 18/40 - Loss: 5.825\n",
      "Iter 18/40 - Loss: 5.674\n",
      "Iter 18/40 - Loss: 5.868\n",
      "Iter 18/40 - Loss: 5.858\n",
      "Iter 18/40 - Loss: 5.751\n",
      "Iter 18/40 - Loss: 5.866\n",
      "Iter 18/40 - Loss: 5.896\n",
      "Iter 18/40 - Loss: 5.981\n",
      "Iter 18/40 - Loss: 5.831\n",
      "Iter 18/40 - Loss: 6.032\n",
      "Iter 18/40 - Loss: 5.872\n",
      "Iter 18/40 - Loss: 5.855\n",
      "Iter 18/40 - Loss: 5.872\n",
      "Iter 19/40 - Loss: 5.907\n",
      "Iter 19/40 - Loss: 5.887\n",
      "Iter 19/40 - Loss: 5.881\n",
      "Iter 19/40 - Loss: 5.952\n",
      "Iter 19/40 - Loss: 5.893\n",
      "Iter 19/40 - Loss: 5.790\n",
      "Iter 19/40 - Loss: 5.940\n",
      "Iter 19/40 - Loss: 5.802\n",
      "Iter 19/40 - Loss: 5.834\n",
      "Iter 19/40 - Loss: 5.814\n",
      "Iter 19/40 - Loss: 6.021\n",
      "Iter 19/40 - Loss: 5.892\n",
      "Iter 19/40 - Loss: 5.793\n",
      "Iter 19/40 - Loss: 5.848\n",
      "Iter 19/40 - Loss: 5.921\n",
      "Iter 19/40 - Loss: 5.789\n",
      "Iter 20/40 - Loss: 5.970\n",
      "Iter 20/40 - Loss: 5.695\n",
      "Iter 20/40 - Loss: 5.894\n",
      "Iter 20/40 - Loss: 5.783\n",
      "Iter 20/40 - Loss: 5.777\n",
      "Iter 20/40 - Loss: 5.898\n",
      "Iter 20/40 - Loss: 5.855\n",
      "Iter 20/40 - Loss: 5.859\n",
      "Iter 20/40 - Loss: 5.994\n",
      "Iter 20/40 - Loss: 5.881\n",
      "Iter 20/40 - Loss: 5.898\n",
      "Iter 20/40 - Loss: 5.976\n",
      "Iter 20/40 - Loss: 5.833\n",
      "Iter 20/40 - Loss: 5.783\n",
      "Iter 20/40 - Loss: 5.912\n",
      "Iter 20/40 - Loss: 5.997\n",
      "Iter 21/40 - Loss: 5.868\n",
      "Iter 21/40 - Loss: 5.799\n",
      "Iter 21/40 - Loss: 5.911\n",
      "Iter 21/40 - Loss: 5.796\n",
      "Iter 21/40 - Loss: 5.868\n",
      "Iter 21/40 - Loss: 5.862\n",
      "Iter 21/40 - Loss: 5.758\n",
      "Iter 21/40 - Loss: 5.899\n",
      "Iter 21/40 - Loss: 5.930\n",
      "Iter 21/40 - Loss: 6.101\n",
      "Iter 21/40 - Loss: 5.912\n",
      "Iter 21/40 - Loss: 5.776\n",
      "Iter 21/40 - Loss: 5.904\n",
      "Iter 21/40 - Loss: 5.783\n",
      "Iter 21/40 - Loss: 5.864\n",
      "Iter 21/40 - Loss: 5.970\n",
      "Iter 22/40 - Loss: 5.846\n",
      "Iter 22/40 - Loss: 5.861\n",
      "Iter 22/40 - Loss: 5.816\n",
      "Iter 22/40 - Loss: 5.777\n",
      "Iter 22/40 - Loss: 5.889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 22/40 - Loss: 5.967\n",
      "Iter 22/40 - Loss: 5.740\n",
      "Iter 22/40 - Loss: 5.819\n",
      "Iter 22/40 - Loss: 5.780\n",
      "Iter 22/40 - Loss: 5.905\n",
      "Iter 22/40 - Loss: 5.873\n",
      "Iter 22/40 - Loss: 6.096\n",
      "Iter 22/40 - Loss: 5.941\n",
      "Iter 22/40 - Loss: 5.955\n",
      "Iter 22/40 - Loss: 5.883\n",
      "Iter 22/40 - Loss: 5.761\n",
      "Iter 23/40 - Loss: 5.946\n",
      "Iter 23/40 - Loss: 5.901\n",
      "Iter 23/40 - Loss: 5.902\n",
      "Iter 23/40 - Loss: 5.846\n",
      "Iter 23/40 - Loss: 5.959\n",
      "Iter 23/40 - Loss: 5.769\n",
      "Iter 23/40 - Loss: 5.855\n",
      "Iter 23/40 - Loss: 5.931\n",
      "Iter 23/40 - Loss: 5.872\n",
      "Iter 23/40 - Loss: 5.831\n",
      "Iter 23/40 - Loss: 5.785\n",
      "Iter 23/40 - Loss: 5.920\n",
      "Iter 23/40 - Loss: 5.856\n",
      "Iter 23/40 - Loss: 5.828\n",
      "Iter 23/40 - Loss: 5.783\n",
      "Iter 23/40 - Loss: 6.061\n",
      "Iter 24/40 - Loss: 5.973\n",
      "Iter 24/40 - Loss: 5.776\n",
      "Iter 24/40 - Loss: 5.817\n",
      "Iter 24/40 - Loss: 5.790\n",
      "Iter 24/40 - Loss: 5.778\n",
      "Iter 24/40 - Loss: 6.038\n",
      "Iter 24/40 - Loss: 5.949\n",
      "Iter 24/40 - Loss: 5.936\n",
      "Iter 24/40 - Loss: 5.813\n",
      "Iter 24/40 - Loss: 5.878\n",
      "Iter 24/40 - Loss: 5.838\n",
      "Iter 24/40 - Loss: 5.747\n",
      "Iter 24/40 - Loss: 5.917\n",
      "Iter 24/40 - Loss: 6.007\n",
      "Iter 24/40 - Loss: 5.776\n",
      "Iter 24/40 - Loss: 5.891\n",
      "Iter 25/40 - Loss: 6.012\n",
      "Iter 25/40 - Loss: 5.898\n",
      "Iter 25/40 - Loss: 5.867\n",
      "Iter 25/40 - Loss: 5.885\n",
      "Iter 25/40 - Loss: 5.864\n",
      "Iter 25/40 - Loss: 5.881\n",
      "Iter 25/40 - Loss: 5.709\n",
      "Iter 25/40 - Loss: 5.884\n",
      "Iter 25/40 - Loss: 5.694\n",
      "Iter 25/40 - Loss: 5.749\n",
      "Iter 25/40 - Loss: 6.152\n",
      "Iter 25/40 - Loss: 5.981\n",
      "Iter 25/40 - Loss: 5.709\n",
      "Iter 25/40 - Loss: 5.875\n",
      "Iter 25/40 - Loss: 5.947\n",
      "Iter 25/40 - Loss: 5.920\n",
      "Iter 26/40 - Loss: 5.868\n",
      "Iter 26/40 - Loss: 5.738\n",
      "Iter 26/40 - Loss: 5.845\n",
      "Iter 26/40 - Loss: 6.034\n",
      "Iter 26/40 - Loss: 5.792\n",
      "Iter 26/40 - Loss: 5.742\n",
      "Iter 26/40 - Loss: 5.874\n",
      "Iter 26/40 - Loss: 6.012\n",
      "Iter 26/40 - Loss: 5.865\n",
      "Iter 26/40 - Loss: 5.890\n",
      "Iter 26/40 - Loss: 5.897\n",
      "Iter 26/40 - Loss: 5.846\n",
      "Iter 26/40 - Loss: 5.818\n",
      "Iter 26/40 - Loss: 5.923\n",
      "Iter 26/40 - Loss: 5.963\n",
      "Iter 26/40 - Loss: 5.833\n",
      "Iter 27/40 - Loss: 5.869\n",
      "Iter 27/40 - Loss: 5.776\n",
      "Iter 27/40 - Loss: 5.945\n",
      "Iter 27/40 - Loss: 5.809\n",
      "Iter 27/40 - Loss: 6.005\n",
      "Iter 27/40 - Loss: 5.758\n",
      "Iter 27/40 - Loss: 5.832\n",
      "Iter 27/40 - Loss: 5.772\n",
      "Iter 27/40 - Loss: 5.943\n",
      "Iter 27/40 - Loss: 5.877\n",
      "Iter 27/40 - Loss: 5.801\n",
      "Iter 27/40 - Loss: 5.849\n",
      "Iter 27/40 - Loss: 5.925\n",
      "Iter 27/40 - Loss: 5.780\n",
      "Iter 27/40 - Loss: 5.877\n",
      "Iter 27/40 - Loss: 6.252\n",
      "Iter 28/40 - Loss: 5.813\n",
      "Iter 28/40 - Loss: 5.821\n",
      "Iter 28/40 - Loss: 5.909\n",
      "Iter 28/40 - Loss: 5.768\n",
      "Iter 28/40 - Loss: 5.869\n",
      "Iter 28/40 - Loss: 5.959\n",
      "Iter 28/40 - Loss: 5.935\n",
      "Iter 28/40 - Loss: 5.932\n",
      "Iter 28/40 - Loss: 5.778\n",
      "Iter 28/40 - Loss: 5.834\n",
      "Iter 28/40 - Loss: 5.903\n",
      "Iter 28/40 - Loss: 5.976\n",
      "Iter 28/40 - Loss: 5.937\n",
      "Iter 28/40 - Loss: 5.824\n",
      "Iter 28/40 - Loss: 5.797\n",
      "Iter 28/40 - Loss: 5.913\n",
      "Iter 29/40 - Loss: 5.911\n",
      "Iter 29/40 - Loss: 5.864\n",
      "Iter 29/40 - Loss: 5.837\n",
      "Iter 29/40 - Loss: 5.851\n",
      "Iter 29/40 - Loss: 5.862\n",
      "Iter 29/40 - Loss: 5.892\n",
      "Iter 29/40 - Loss: 5.798\n",
      "Iter 29/40 - Loss: 5.977\n",
      "Iter 29/40 - Loss: 5.842\n",
      "Iter 29/40 - Loss: 5.961\n",
      "Iter 29/40 - Loss: 5.799\n",
      "Iter 29/40 - Loss: 5.942\n",
      "Iter 29/40 - Loss: 5.778\n",
      "Iter 29/40 - Loss: 5.772\n",
      "Iter 29/40 - Loss: 6.043\n",
      "Iter 29/40 - Loss: 5.743\n",
      "Iter 30/40 - Loss: 5.991\n",
      "Iter 30/40 - Loss: 5.921\n",
      "Iter 30/40 - Loss: 5.875\n",
      "Iter 30/40 - Loss: 5.959\n",
      "Iter 30/40 - Loss: 5.830\n",
      "Iter 30/40 - Loss: 5.810\n",
      "Iter 30/40 - Loss: 6.014\n",
      "Iter 30/40 - Loss: 5.846\n",
      "Iter 30/40 - Loss: 5.705\n",
      "Iter 30/40 - Loss: 5.804\n",
      "Iter 30/40 - Loss: 5.900\n",
      "Iter 30/40 - Loss: 5.985\n",
      "Iter 30/40 - Loss: 5.978\n",
      "Iter 30/40 - Loss: 5.739\n",
      "Iter 30/40 - Loss: 5.700\n",
      "Iter 30/40 - Loss: 5.879\n",
      "Iter 31/40 - Loss: 5.884\n",
      "Iter 31/40 - Loss: 6.025\n",
      "Iter 31/40 - Loss: 5.902\n",
      "Iter 31/40 - Loss: 5.811\n",
      "Iter 31/40 - Loss: 5.819\n",
      "Iter 31/40 - Loss: 5.846\n",
      "Iter 31/40 - Loss: 5.782\n",
      "Iter 31/40 - Loss: 5.878\n",
      "Iter 31/40 - Loss: 5.776\n",
      "Iter 31/40 - Loss: 5.806\n",
      "Iter 31/40 - Loss: 5.906\n",
      "Iter 31/40 - Loss: 5.847\n",
      "Iter 31/40 - Loss: 5.793\n",
      "Iter 31/40 - Loss: 6.004\n",
      "Iter 31/40 - Loss: 5.832\n",
      "Iter 31/40 - Loss: 6.011\n",
      "Iter 32/40 - Loss: 5.874\n",
      "Iter 32/40 - Loss: 6.097\n",
      "Iter 32/40 - Loss: 5.899\n",
      "Iter 32/40 - Loss: 5.860\n",
      "Iter 32/40 - Loss: 5.977\n",
      "Iter 32/40 - Loss: 5.998\n",
      "Iter 32/40 - Loss: 5.679\n",
      "Iter 32/40 - Loss: 5.863\n",
      "Iter 32/40 - Loss: 5.854\n",
      "Iter 32/40 - Loss: 5.884\n",
      "Iter 32/40 - Loss: 5.803\n",
      "Iter 32/40 - Loss: 5.758\n",
      "Iter 32/40 - Loss: 5.820\n",
      "Iter 32/40 - Loss: 5.823\n",
      "Iter 32/40 - Loss: 5.749\n",
      "Iter 32/40 - Loss: 5.901\n",
      "Iter 33/40 - Loss: 5.968\n",
      "Iter 33/40 - Loss: 5.864\n",
      "Iter 33/40 - Loss: 5.793\n",
      "Iter 33/40 - Loss: 5.764\n",
      "Iter 33/40 - Loss: 5.922\n",
      "Iter 33/40 - Loss: 5.906\n",
      "Iter 33/40 - Loss: 5.936\n",
      "Iter 33/40 - Loss: 5.835\n",
      "Iter 33/40 - Loss: 5.833\n",
      "Iter 33/40 - Loss: 5.849\n",
      "Iter 33/40 - Loss: 5.724\n",
      "Iter 33/40 - Loss: 5.897\n",
      "Iter 33/40 - Loss: 5.905\n",
      "Iter 33/40 - Loss: 5.906\n",
      "Iter 33/40 - Loss: 5.894\n",
      "Iter 33/40 - Loss: 5.769\n",
      "Iter 34/40 - Loss: 5.819\n",
      "Iter 34/40 - Loss: 5.842\n",
      "Iter 34/40 - Loss: 5.900\n",
      "Iter 34/40 - Loss: 6.041\n",
      "Iter 34/40 - Loss: 5.779\n",
      "Iter 34/40 - Loss: 5.819\n",
      "Iter 34/40 - Loss: 5.840\n",
      "Iter 34/40 - Loss: 5.845\n",
      "Iter 34/40 - Loss: 5.882\n",
      "Iter 34/40 - Loss: 5.808\n",
      "Iter 34/40 - Loss: 5.809\n",
      "Iter 34/40 - Loss: 5.876\n",
      "Iter 34/40 - Loss: 5.871\n",
      "Iter 34/40 - Loss: 5.984\n",
      "Iter 34/40 - Loss: 5.811\n",
      "Iter 34/40 - Loss: 5.875\n",
      "Iter 35/40 - Loss: 5.841\n",
      "Iter 35/40 - Loss: 5.892\n",
      "Iter 35/40 - Loss: 5.759\n",
      "Iter 35/40 - Loss: 5.742\n",
      "Iter 35/40 - Loss: 6.043\n",
      "Iter 35/40 - Loss: 5.776\n",
      "Iter 35/40 - Loss: 5.862\n",
      "Iter 35/40 - Loss: 5.836\n",
      "Iter 35/40 - Loss: 5.799\n",
      "Iter 35/40 - Loss: 5.848\n",
      "Iter 35/40 - Loss: 5.973\n",
      "Iter 35/40 - Loss: 5.833\n",
      "Iter 35/40 - Loss: 5.797\n",
      "Iter 35/40 - Loss: 5.857\n",
      "Iter 35/40 - Loss: 5.982\n",
      "Iter 35/40 - Loss: 5.974\n",
      "Iter 36/40 - Loss: 5.885\n",
      "Iter 36/40 - Loss: 5.884\n",
      "Iter 36/40 - Loss: 5.785\n",
      "Iter 36/40 - Loss: 5.832\n",
      "Iter 36/40 - Loss: 5.760\n",
      "Iter 36/40 - Loss: 5.943\n",
      "Iter 36/40 - Loss: 5.792\n",
      "Iter 36/40 - Loss: 5.975\n",
      "Iter 36/40 - Loss: 5.931\n",
      "Iter 36/40 - Loss: 5.837\n",
      "Iter 36/40 - Loss: 5.819\n",
      "Iter 36/40 - Loss: 5.958\n",
      "Iter 36/40 - Loss: 5.751\n",
      "Iter 36/40 - Loss: 5.915\n",
      "Iter 36/40 - Loss: 5.767\n",
      "Iter 36/40 - Loss: 5.981\n",
      "Iter 37/40 - Loss: 5.881\n",
      "Iter 37/40 - Loss: 5.916\n",
      "Iter 37/40 - Loss: 5.952\n",
      "Iter 37/40 - Loss: 5.820\n",
      "Iter 37/40 - Loss: 5.727\n",
      "Iter 37/40 - Loss: 5.825\n",
      "Iter 37/40 - Loss: 5.934\n",
      "Iter 37/40 - Loss: 6.000\n",
      "Iter 37/40 - Loss: 5.915\n",
      "Iter 37/40 - Loss: 5.848\n",
      "Iter 37/40 - Loss: 5.903\n",
      "Iter 37/40 - Loss: 5.786\n",
      "Iter 37/40 - Loss: 5.713\n",
      "Iter 37/40 - Loss: 5.806\n",
      "Iter 37/40 - Loss: 5.883\n",
      "Iter 37/40 - Loss: 5.849\n",
      "Iter 38/40 - Loss: 5.826\n",
      "Iter 38/40 - Loss: 5.825\n",
      "Iter 38/40 - Loss: 5.894\n",
      "Iter 38/40 - Loss: 5.907\n",
      "Iter 38/40 - Loss: 5.866\n",
      "Iter 38/40 - Loss: 5.938\n",
      "Iter 38/40 - Loss: 5.826\n",
      "Iter 38/40 - Loss: 5.934\n",
      "Iter 38/40 - Loss: 5.659\n",
      "Iter 38/40 - Loss: 5.897\n",
      "Iter 38/40 - Loss: 5.822\n",
      "Iter 38/40 - Loss: 5.892\n",
      "Iter 38/40 - Loss: 5.792\n",
      "Iter 38/40 - Loss: 5.823\n",
      "Iter 38/40 - Loss: 5.883\n",
      "Iter 38/40 - Loss: 6.070\n",
      "Iter 39/40 - Loss: 5.824\n",
      "Iter 39/40 - Loss: 5.868\n",
      "Iter 39/40 - Loss: 5.929\n",
      "Iter 39/40 - Loss: 5.836\n",
      "Iter 39/40 - Loss: 5.704\n",
      "Iter 39/40 - Loss: 5.814\n",
      "Iter 39/40 - Loss: 5.940\n",
      "Iter 39/40 - Loss: 5.716\n",
      "Iter 39/40 - Loss: 6.057\n",
      "Iter 39/40 - Loss: 5.880\n",
      "Iter 39/40 - Loss: 5.779\n",
      "Iter 39/40 - Loss: 5.726\n",
      "Iter 39/40 - Loss: 5.840\n",
      "Iter 39/40 - Loss: 5.987\n",
      "Iter 39/40 - Loss: 5.969\n",
      "Iter 39/40 - Loss: 5.894\n",
      "Iter 40/40 - Loss: 5.894\n",
      "Iter 40/40 - Loss: 5.882\n",
      "Iter 40/40 - Loss: 5.897\n",
      "Iter 40/40 - Loss: 5.932\n",
      "Iter 40/40 - Loss: 5.817\n",
      "Iter 40/40 - Loss: 5.907\n",
      "Iter 40/40 - Loss: 5.824\n",
      "Iter 40/40 - Loss: 5.800\n",
      "Iter 40/40 - Loss: 5.904\n",
      "Iter 40/40 - Loss: 5.841\n",
      "Iter 40/40 - Loss: 5.831\n",
      "Iter 40/40 - Loss: 5.836\n",
      "Iter 40/40 - Loss: 5.803\n",
      "Iter 40/40 - Loss: 5.857\n",
      "Iter 40/40 - Loss: 5.840\n",
      "Iter 40/40 - Loss: 5.918\n",
      "CPU times: user 42 s, sys: 48 s, total: 1min 30s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 40 iterations of optimization\n",
    "n_iter = 40\n",
    "\n",
    "# We use SGD here, rather than Adam\n",
    "# Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# We use a Learning rate scheduler from PyTorch to lower the learning rate during optimization\n",
    "# We're going to drop the learning rate by 1/10 after 3/4 of training\n",
    "# This helps the model converge to a minimum\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.75 * n_iter], gamma=0.1)\n",
    "\n",
    "# Our loss object\n",
    "# We're using the VariationalMarginalLogLikelihood object\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "# The training loop\n",
    "def train():\n",
    "    for i in range(n_iter):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # We're going to use two context managers here\n",
    "            \n",
    "            # The use_toeplitz flag makes learning faster on the GPU (trades off memory for speed)\n",
    "            # The diagonal_correction flag improves the approximations we're making for variational inference\n",
    "            # It makes running time a bit slower, but improves the optimization and predictions\n",
    "            with gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "                output = model(x_batch)\n",
    "                loss = -mll(output, y_batch)\n",
    "                print('Iter %d/%d - Loss: %.3f' % (i + 1, n_iter, loss.item()))\n",
    "            \n",
    "            # The actual optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0af4030ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = torch.linspace(0, 1, 51)\n",
    "test_y = torch.sin(test_x * (4 * math.pi))\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(test_x.detach().cpu().numpy(), test_y.detach().cpu().numpy(), 'k*')\n",
    "ax.plot(test_x.detach().cpu().numpy(), observed_pred.mean.detach().cpu().numpy(), 'b')\n",
    "ax.fill_between(test_x.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy(), alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
