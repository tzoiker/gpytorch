{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to perform GP regression, but using **variational inference** rather than exact inference. There are a few cases where variational inference may be prefereable:\n",
    "\n",
    "1) If you have lots of data, and want to perform **stochastic optimization**\n",
    "\n",
    "2) If you have a model where you want to use other variational distributions\n",
    "\n",
    "KISS-GP with SVI was introduced in:\n",
    "https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a training set\n",
    "# We're going to learn a sine function\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "train_y = torch.sin(train_x * (4 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SGD - the dataloader\n",
    "\n",
    "Because we want to do stochastic optimization, we have to put the dataset in a pytorch **DataLoader**.\n",
    "This creates easy minibatches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "This is pretty similar to a normal regression model, except now we're using a `gpytorch.models.GridInducingVariationalGP` instead of a `gpytorch.models.ExactGP`.\n",
    "\n",
    "Any of the variational models would work. We're using the `GridInducingVariationalGP` because we have many data points, but only 1 dimensional data.\n",
    "\n",
    "Similar to exact regression, we use a `GaussianLikelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.priors import SmoothedBoxPrior\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        super(GPRegressionModel, self).__init__(grid_size=20, grid_bounds=[(-0.05, 1.05)])\n",
    "        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-10, 10))\n",
    "        self.covar_module = RBFKernel(\n",
    "            log_lengthscale_prior=SmoothedBoxPrior(math.exp(-3), math.exp(6), sigma=0.1, log_transform=True)\n",
    "        )\n",
    "        self.register_parameter(\n",
    "            name=\"log_outputscale\",\n",
    "            parameter=torch.nn.Parameter(torch.Tensor([0])),\n",
    "            prior=SmoothedBoxPrior(math.exp(-5), math.exp(1), sigma=0.1, log_transform=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x) * self.log_outputscale.exp()\n",
    "        return GaussianRandomVariable(mean_x, covar_x)\n",
    "    \n",
    "model = GPRegressionModel()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "This training loop will use **stochastic optimization** rather than batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jrg365/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/40 - Loss: 20.399 (0.100)\n",
      "Iter 1/40 - Loss: 18.651 (0.100)\n",
      "Iter 1/40 - Loss: 17.317 (0.100)\n",
      "Iter 1/40 - Loss: 16.199 (0.100)\n",
      "Iter 1/40 - Loss: 15.267 (0.100)\n",
      "Iter 1/40 - Loss: 14.627 (0.100)\n",
      "Iter 1/40 - Loss: 13.976 (0.100)\n",
      "Iter 1/40 - Loss: 13.521 (0.100)\n",
      "Iter 1/40 - Loss: 13.101 (0.100)\n",
      "Iter 1/40 - Loss: 12.819 (0.100)\n",
      "Iter 1/40 - Loss: 12.504 (0.100)\n",
      "Iter 1/40 - Loss: 12.258 (0.100)\n",
      "Iter 1/40 - Loss: 12.232 (0.100)\n",
      "Iter 1/40 - Loss: 12.020 (0.100)\n",
      "Iter 1/40 - Loss: 11.863 (0.100)\n",
      "Iter 1/40 - Loss: 11.756 (0.100)\n",
      "Iter 2/40 - Loss: 11.702 (0.100)\n",
      "Iter 2/40 - Loss: 11.655 (0.100)\n",
      "Iter 2/40 - Loss: 11.561 (0.100)\n",
      "Iter 2/40 - Loss: 11.501 (0.100)\n",
      "Iter 2/40 - Loss: 11.488 (0.100)\n",
      "Iter 2/40 - Loss: 11.455 (0.100)\n",
      "Iter 2/40 - Loss: 11.411 (0.100)\n",
      "Iter 2/40 - Loss: 11.325 (0.100)\n",
      "Iter 2/40 - Loss: 11.346 (0.100)\n",
      "Iter 2/40 - Loss: 11.296 (0.100)\n",
      "Iter 2/40 - Loss: 11.341 (0.100)\n",
      "Iter 2/40 - Loss: 11.316 (0.100)\n",
      "Iter 2/40 - Loss: 11.307 (0.100)\n",
      "Iter 2/40 - Loss: 11.211 (0.100)\n",
      "Iter 2/40 - Loss: 11.288 (0.100)\n",
      "Iter 2/40 - Loss: 11.237 (0.100)\n",
      "Iter 3/40 - Loss: 11.242 (0.100)\n",
      "Iter 3/40 - Loss: 11.251 (0.100)\n",
      "Iter 3/40 - Loss: 11.229 (0.100)\n",
      "Iter 3/40 - Loss: 11.172 (0.100)\n",
      "Iter 3/40 - Loss: 11.165 (0.100)\n",
      "Iter 3/40 - Loss: 11.169 (0.100)\n",
      "Iter 3/40 - Loss: 11.150 (0.100)\n",
      "Iter 3/40 - Loss: 11.094 (0.100)\n",
      "Iter 3/40 - Loss: 11.179 (0.100)\n",
      "Iter 3/40 - Loss: 11.093 (0.100)\n",
      "Iter 3/40 - Loss: 11.157 (0.100)\n",
      "Iter 3/40 - Loss: 11.176 (0.100)\n",
      "Iter 3/40 - Loss: 11.121 (0.100)\n",
      "Iter 3/40 - Loss: 11.134 (0.100)\n",
      "Iter 3/40 - Loss: 11.176 (0.100)\n",
      "Iter 3/40 - Loss: 11.084 (0.100)\n",
      "Iter 4/40 - Loss: 11.014 (0.100)\n",
      "Iter 4/40 - Loss: 11.120 (0.100)\n",
      "Iter 4/40 - Loss: 11.082 (0.100)\n",
      "Iter 4/40 - Loss: 11.068 (0.100)\n",
      "Iter 4/40 - Loss: 11.036 (0.100)\n",
      "Iter 4/40 - Loss: 11.054 (0.100)\n",
      "Iter 4/40 - Loss: 11.025 (0.100)\n",
      "Iter 4/40 - Loss: 11.062 (0.100)\n",
      "Iter 4/40 - Loss: 11.047 (0.100)\n",
      "Iter 4/40 - Loss: 11.085 (0.100)\n",
      "Iter 4/40 - Loss: 10.945 (0.100)\n",
      "Iter 4/40 - Loss: 10.957 (0.100)\n",
      "Iter 4/40 - Loss: 10.939 (0.100)\n",
      "Iter 4/40 - Loss: 10.988 (0.100)\n",
      "Iter 4/40 - Loss: 10.954 (0.100)\n",
      "Iter 4/40 - Loss: 10.983 (0.100)\n",
      "Iter 5/40 - Loss: 10.914 (0.100)\n",
      "Iter 5/40 - Loss: 10.887 (0.100)\n",
      "Iter 5/40 - Loss: 10.884 (0.100)\n",
      "Iter 5/40 - Loss: 10.924 (0.100)\n",
      "Iter 5/40 - Loss: 10.953 (0.100)\n",
      "Iter 5/40 - Loss: 10.903 (0.100)\n",
      "Iter 5/40 - Loss: 10.901 (0.100)\n",
      "Iter 5/40 - Loss: 10.860 (0.100)\n",
      "Iter 5/40 - Loss: 10.911 (0.100)\n",
      "Iter 5/40 - Loss: 10.837 (0.100)\n",
      "Iter 5/40 - Loss: 10.847 (0.100)\n",
      "Iter 5/40 - Loss: 10.816 (0.100)\n",
      "Iter 5/40 - Loss: 10.813 (0.100)\n",
      "Iter 5/40 - Loss: 10.802 (0.100)\n",
      "Iter 5/40 - Loss: 10.750 (0.100)\n",
      "Iter 5/40 - Loss: 10.724 (0.100)\n",
      "Iter 6/40 - Loss: 10.781 (0.100)\n",
      "Iter 6/40 - Loss: 10.723 (0.100)\n",
      "Iter 6/40 - Loss: 10.747 (0.100)\n",
      "Iter 6/40 - Loss: 10.711 (0.100)\n",
      "Iter 6/40 - Loss: 10.728 (0.100)\n",
      "Iter 6/40 - Loss: 10.659 (0.100)\n",
      "Iter 6/40 - Loss: 10.693 (0.100)\n",
      "Iter 6/40 - Loss: 10.683 (0.100)\n",
      "Iter 6/40 - Loss: 10.710 (0.100)\n",
      "Iter 6/40 - Loss: 10.566 (0.100)\n",
      "Iter 6/40 - Loss: 10.703 (0.100)\n",
      "Iter 6/40 - Loss: 10.637 (0.100)\n",
      "Iter 6/40 - Loss: 10.602 (0.100)\n",
      "Iter 6/40 - Loss: 10.563 (0.100)\n",
      "Iter 6/40 - Loss: 10.557 (0.100)\n",
      "Iter 6/40 - Loss: 10.617 (0.100)\n",
      "Iter 7/40 - Loss: 10.498 (0.100)\n",
      "Iter 7/40 - Loss: 10.555 (0.100)\n",
      "Iter 7/40 - Loss: 10.500 (0.100)\n",
      "Iter 7/40 - Loss: 10.546 (0.100)\n",
      "Iter 7/40 - Loss: 10.444 (0.100)\n",
      "Iter 7/40 - Loss: 10.435 (0.100)\n",
      "Iter 7/40 - Loss: 10.508 (0.100)\n",
      "Iter 7/40 - Loss: 10.546 (0.100)\n",
      "Iter 7/40 - Loss: 10.446 (0.100)\n",
      "Iter 7/40 - Loss: 10.429 (0.100)\n",
      "Iter 7/40 - Loss: 10.378 (0.100)\n",
      "Iter 7/40 - Loss: 10.343 (0.100)\n",
      "Iter 7/40 - Loss: 10.397 (0.100)\n",
      "Iter 7/40 - Loss: 10.417 (0.100)\n",
      "Iter 7/40 - Loss: 10.329 (0.100)\n",
      "Iter 7/40 - Loss: 10.360 (0.100)\n",
      "Iter 8/40 - Loss: 10.376 (0.100)\n",
      "Iter 8/40 - Loss: 10.297 (0.100)\n",
      "Iter 8/40 - Loss: 10.291 (0.100)\n",
      "Iter 8/40 - Loss: 10.321 (0.100)\n",
      "Iter 8/40 - Loss: 10.254 (0.100)\n",
      "Iter 8/40 - Loss: 10.253 (0.100)\n",
      "Iter 8/40 - Loss: 10.228 (0.100)\n",
      "Iter 8/40 - Loss: 10.205 (0.100)\n",
      "Iter 8/40 - Loss: 10.167 (0.100)\n",
      "Iter 8/40 - Loss: 10.185 (0.100)\n",
      "Iter 8/40 - Loss: 10.195 (0.100)\n",
      "Iter 8/40 - Loss: 10.272 (0.100)\n",
      "Iter 8/40 - Loss: 10.151 (0.100)\n",
      "Iter 8/40 - Loss: 10.172 (0.100)\n",
      "Iter 8/40 - Loss: 10.163 (0.100)\n",
      "Iter 8/40 - Loss: 10.147 (0.100)\n",
      "Iter 9/40 - Loss: 10.148 (0.100)\n",
      "Iter 9/40 - Loss: 10.123 (0.100)\n",
      "Iter 9/40 - Loss: 10.088 (0.100)\n",
      "Iter 9/40 - Loss: 10.151 (0.100)\n",
      "Iter 9/40 - Loss: 10.093 (0.100)\n",
      "Iter 9/40 - Loss: 10.158 (0.100)\n",
      "Iter 9/40 - Loss: 10.076 (0.100)\n",
      "Iter 9/40 - Loss: 10.069 (0.100)\n",
      "Iter 9/40 - Loss: 10.041 (0.100)\n",
      "Iter 9/40 - Loss: 10.033 (0.100)\n",
      "Iter 9/40 - Loss: 10.061 (0.100)\n",
      "Iter 9/40 - Loss: 10.051 (0.100)\n",
      "Iter 9/40 - Loss: 10.053 (0.100)\n",
      "Iter 9/40 - Loss: 10.026 (0.100)\n",
      "Iter 9/40 - Loss: 10.031 (0.100)\n",
      "Iter 9/40 - Loss: 9.961 (0.100)\n",
      "Iter 10/40 - Loss: 10.147 (0.100)\n",
      "Iter 10/40 - Loss: 10.035 (0.100)\n",
      "Iter 10/40 - Loss: 10.021 (0.100)\n",
      "Iter 10/40 - Loss: 10.027 (0.100)\n",
      "Iter 10/40 - Loss: 10.033 (0.100)\n",
      "Iter 10/40 - Loss: 10.018 (0.100)\n",
      "Iter 10/40 - Loss: 9.958 (0.100)\n",
      "Iter 10/40 - Loss: 9.907 (0.100)\n",
      "Iter 10/40 - Loss: 9.988 (0.100)\n",
      "Iter 10/40 - Loss: 9.942 (0.100)\n",
      "Iter 10/40 - Loss: 9.996 (0.100)\n",
      "Iter 10/40 - Loss: 10.056 (0.100)\n",
      "Iter 10/40 - Loss: 9.978 (0.100)\n",
      "Iter 10/40 - Loss: 9.952 (0.100)\n",
      "Iter 10/40 - Loss: 9.940 (0.100)\n",
      "Iter 10/40 - Loss: 9.974 (0.100)\n",
      "Iter 11/40 - Loss: 10.004 (0.100)\n",
      "Iter 11/40 - Loss: 9.940 (0.100)\n",
      "Iter 11/40 - Loss: 9.957 (0.100)\n",
      "Iter 11/40 - Loss: 9.951 (0.100)\n",
      "Iter 11/40 - Loss: 9.936 (0.100)\n",
      "Iter 11/40 - Loss: 10.032 (0.100)\n",
      "Iter 11/40 - Loss: 9.929 (0.100)\n",
      "Iter 11/40 - Loss: 10.019 (0.100)\n",
      "Iter 11/40 - Loss: 9.975 (0.100)\n",
      "Iter 11/40 - Loss: 10.061 (0.100)\n",
      "Iter 11/40 - Loss: 9.902 (0.100)\n",
      "Iter 11/40 - Loss: 9.967 (0.100)\n",
      "Iter 11/40 - Loss: 10.053 (0.100)\n",
      "Iter 11/40 - Loss: 9.971 (0.100)\n",
      "Iter 11/40 - Loss: 9.864 (0.100)\n",
      "Iter 11/40 - Loss: 9.986 (0.100)\n",
      "Iter 12/40 - Loss: 9.900 (0.100)\n",
      "Iter 12/40 - Loss: 9.904 (0.100)\n",
      "Iter 12/40 - Loss: 9.935 (0.100)\n",
      "Iter 12/40 - Loss: 10.026 (0.100)\n",
      "Iter 12/40 - Loss: 9.920 (0.100)\n",
      "Iter 12/40 - Loss: 10.056 (0.100)\n",
      "Iter 12/40 - Loss: 10.034 (0.100)\n",
      "Iter 12/40 - Loss: 9.974 (0.100)\n",
      "Iter 12/40 - Loss: 9.986 (0.100)\n",
      "Iter 12/40 - Loss: 9.964 (0.100)\n",
      "Iter 12/40 - Loss: 9.916 (0.100)\n",
      "Iter 12/40 - Loss: 9.934 (0.100)\n",
      "Iter 12/40 - Loss: 10.023 (0.100)\n",
      "Iter 12/40 - Loss: 9.961 (0.100)\n",
      "Iter 12/40 - Loss: 9.979 (0.100)\n",
      "Iter 12/40 - Loss: 9.931 (0.100)\n",
      "Iter 13/40 - Loss: 9.923 (0.100)\n",
      "Iter 13/40 - Loss: 9.914 (0.100)\n",
      "Iter 13/40 - Loss: 9.949 (0.100)\n",
      "Iter 13/40 - Loss: 10.027 (0.100)\n",
      "Iter 13/40 - Loss: 9.928 (0.100)\n",
      "Iter 13/40 - Loss: 9.933 (0.100)\n",
      "Iter 13/40 - Loss: 9.886 (0.100)\n",
      "Iter 13/40 - Loss: 9.956 (0.100)\n",
      "Iter 13/40 - Loss: 10.081 (0.100)\n",
      "Iter 13/40 - Loss: 9.889 (0.100)\n",
      "Iter 13/40 - Loss: 10.066 (0.100)\n",
      "Iter 13/40 - Loss: 9.845 (0.100)\n",
      "Iter 13/40 - Loss: 9.921 (0.100)\n",
      "Iter 13/40 - Loss: 10.078 (0.100)\n",
      "Iter 13/40 - Loss: 10.016 (0.100)\n",
      "Iter 13/40 - Loss: 9.931 (0.100)\n",
      "Iter 14/40 - Loss: 10.034 (0.100)\n",
      "Iter 14/40 - Loss: 9.925 (0.100)\n",
      "Iter 14/40 - Loss: 9.983 (0.100)\n",
      "Iter 14/40 - Loss: 10.009 (0.100)\n",
      "Iter 14/40 - Loss: 9.882 (0.100)\n",
      "Iter 14/40 - Loss: 9.903 (0.100)\n",
      "Iter 14/40 - Loss: 9.926 (0.100)\n",
      "Iter 14/40 - Loss: 9.915 (0.100)\n",
      "Iter 14/40 - Loss: 10.081 (0.100)\n",
      "Iter 14/40 - Loss: 9.926 (0.100)\n",
      "Iter 14/40 - Loss: 9.977 (0.100)\n",
      "Iter 14/40 - Loss: 9.954 (0.100)\n",
      "Iter 14/40 - Loss: 9.843 (0.100)\n",
      "Iter 14/40 - Loss: 9.994 (0.100)\n",
      "Iter 14/40 - Loss: 10.080 (0.100)\n",
      "Iter 14/40 - Loss: 9.890 (0.100)\n",
      "Iter 15/40 - Loss: 9.994 (0.100)\n",
      "Iter 15/40 - Loss: 9.924 (0.100)\n",
      "Iter 15/40 - Loss: 9.787 (0.100)\n",
      "Iter 15/40 - Loss: 10.102 (0.100)\n",
      "Iter 15/40 - Loss: 9.857 (0.100)\n",
      "Iter 15/40 - Loss: 9.820 (0.100)\n",
      "Iter 15/40 - Loss: 9.888 (0.100)\n",
      "Iter 15/40 - Loss: 10.004 (0.100)\n",
      "Iter 15/40 - Loss: 10.033 (0.100)\n",
      "Iter 15/40 - Loss: 10.057 (0.100)\n",
      "Iter 15/40 - Loss: 10.048 (0.100)\n",
      "Iter 15/40 - Loss: 9.960 (0.100)\n",
      "Iter 15/40 - Loss: 10.069 (0.100)\n",
      "Iter 15/40 - Loss: 9.864 (0.100)\n",
      "Iter 15/40 - Loss: 10.079 (0.100)\n",
      "Iter 15/40 - Loss: 9.839 (0.100)\n",
      "Iter 16/40 - Loss: 9.977 (0.100)\n",
      "Iter 16/40 - Loss: 9.951 (0.100)\n",
      "Iter 16/40 - Loss: 9.974 (0.100)\n",
      "Iter 16/40 - Loss: 9.954 (0.100)\n",
      "Iter 16/40 - Loss: 9.980 (0.100)\n",
      "Iter 16/40 - Loss: 10.002 (0.100)\n",
      "Iter 16/40 - Loss: 9.834 (0.100)\n",
      "Iter 16/40 - Loss: 9.949 (0.100)\n",
      "Iter 16/40 - Loss: 9.929 (0.100)\n",
      "Iter 16/40 - Loss: 9.853 (0.100)\n",
      "Iter 16/40 - Loss: 10.144 (0.100)\n",
      "Iter 16/40 - Loss: 10.026 (0.100)\n",
      "Iter 16/40 - Loss: 10.008 (0.100)\n",
      "Iter 16/40 - Loss: 9.989 (0.100)\n",
      "Iter 16/40 - Loss: 9.897 (0.100)\n",
      "Iter 16/40 - Loss: 9.871 (0.100)\n",
      "Iter 17/40 - Loss: 9.886 (0.100)\n",
      "Iter 17/40 - Loss: 10.004 (0.100)\n",
      "Iter 17/40 - Loss: 9.865 (0.100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 17/40 - Loss: 10.035 (0.100)\n",
      "Iter 17/40 - Loss: 10.028 (0.100)\n",
      "Iter 17/40 - Loss: 9.986 (0.100)\n",
      "Iter 17/40 - Loss: 9.822 (0.100)\n",
      "Iter 17/40 - Loss: 10.061 (0.100)\n",
      "Iter 17/40 - Loss: 10.055 (0.100)\n",
      "Iter 17/40 - Loss: 9.932 (0.100)\n",
      "Iter 17/40 - Loss: 9.985 (0.100)\n",
      "Iter 17/40 - Loss: 9.988 (0.100)\n",
      "Iter 17/40 - Loss: 9.888 (0.100)\n",
      "Iter 17/40 - Loss: 10.004 (0.100)\n",
      "Iter 17/40 - Loss: 9.948 (0.100)\n",
      "Iter 17/40 - Loss: 9.887 (0.100)\n",
      "Iter 18/40 - Loss: 9.961 (0.100)\n",
      "Iter 18/40 - Loss: 10.050 (0.100)\n",
      "Iter 18/40 - Loss: 9.802 (0.100)\n",
      "Iter 18/40 - Loss: 9.963 (0.100)\n",
      "Iter 18/40 - Loss: 9.967 (0.100)\n",
      "Iter 18/40 - Loss: 9.827 (0.100)\n",
      "Iter 18/40 - Loss: 10.036 (0.100)\n",
      "Iter 18/40 - Loss: 9.958 (0.100)\n",
      "Iter 18/40 - Loss: 10.111 (0.100)\n",
      "Iter 18/40 - Loss: 9.958 (0.100)\n",
      "Iter 18/40 - Loss: 9.869 (0.100)\n",
      "Iter 18/40 - Loss: 9.969 (0.100)\n",
      "Iter 18/40 - Loss: 9.920 (0.100)\n",
      "Iter 18/40 - Loss: 9.987 (0.100)\n",
      "Iter 18/40 - Loss: 9.947 (0.100)\n",
      "Iter 18/40 - Loss: 10.138 (0.100)\n",
      "Iter 19/40 - Loss: 9.940 (0.100)\n",
      "Iter 19/40 - Loss: 9.876 (0.100)\n",
      "Iter 19/40 - Loss: 9.844 (0.100)\n",
      "Iter 19/40 - Loss: 10.034 (0.100)\n",
      "Iter 19/40 - Loss: 9.921 (0.100)\n",
      "Iter 19/40 - Loss: 9.903 (0.100)\n",
      "Iter 19/40 - Loss: 9.855 (0.100)\n",
      "Iter 19/40 - Loss: 10.029 (0.100)\n",
      "Iter 19/40 - Loss: 9.933 (0.100)\n",
      "Iter 19/40 - Loss: 9.800 (0.100)\n",
      "Iter 19/40 - Loss: 9.851 (0.100)\n",
      "Iter 19/40 - Loss: 9.922 (0.100)\n",
      "Iter 19/40 - Loss: 10.179 (0.100)\n",
      "Iter 19/40 - Loss: 10.048 (0.100)\n",
      "Iter 19/40 - Loss: 10.088 (0.100)\n",
      "Iter 19/40 - Loss: 10.285 (0.100)\n",
      "Iter 20/40 - Loss: 9.946 (0.100)\n",
      "Iter 20/40 - Loss: 9.923 (0.100)\n",
      "Iter 20/40 - Loss: 10.042 (0.100)\n",
      "Iter 20/40 - Loss: 10.058 (0.100)\n",
      "Iter 20/40 - Loss: 9.961 (0.100)\n",
      "Iter 20/40 - Loss: 10.026 (0.100)\n",
      "Iter 20/40 - Loss: 9.985 (0.100)\n",
      "Iter 20/40 - Loss: 9.836 (0.100)\n",
      "Iter 20/40 - Loss: 9.929 (0.100)\n",
      "Iter 20/40 - Loss: 10.161 (0.100)\n",
      "Iter 20/40 - Loss: 9.883 (0.100)\n",
      "Iter 20/40 - Loss: 9.884 (0.100)\n",
      "Iter 20/40 - Loss: 9.940 (0.100)\n",
      "Iter 20/40 - Loss: 9.820 (0.100)\n",
      "Iter 20/40 - Loss: 10.036 (0.100)\n",
      "Iter 20/40 - Loss: 9.977 (0.100)\n",
      "Iter 21/40 - Loss: 9.954 (0.100)\n",
      "Iter 21/40 - Loss: 9.954 (0.100)\n",
      "Iter 21/40 - Loss: 9.874 (0.100)\n",
      "Iter 21/40 - Loss: 10.046 (0.100)\n",
      "Iter 21/40 - Loss: 10.107 (0.100)\n",
      "Iter 21/40 - Loss: 9.964 (0.100)\n",
      "Iter 21/40 - Loss: 9.864 (0.100)\n",
      "Iter 21/40 - Loss: 10.007 (0.100)\n",
      "Iter 21/40 - Loss: 9.824 (0.100)\n",
      "Iter 21/40 - Loss: 10.009 (0.100)\n",
      "Iter 21/40 - Loss: 9.974 (0.100)\n",
      "Iter 21/40 - Loss: 9.874 (0.100)\n",
      "Iter 21/40 - Loss: 9.849 (0.100)\n",
      "Iter 21/40 - Loss: 9.987 (0.100)\n",
      "Iter 21/40 - Loss: 9.975 (0.100)\n",
      "Iter 21/40 - Loss: 10.220 (0.100)\n",
      "Iter 22/40 - Loss: 10.051 (0.100)\n",
      "Iter 22/40 - Loss: 9.909 (0.100)\n",
      "Iter 22/40 - Loss: 9.965 (0.100)\n",
      "Iter 22/40 - Loss: 9.938 (0.100)\n",
      "Iter 22/40 - Loss: 10.032 (0.100)\n",
      "Iter 22/40 - Loss: 9.960 (0.100)\n",
      "Iter 22/40 - Loss: 9.829 (0.100)\n",
      "Iter 22/40 - Loss: 9.977 (0.100)\n",
      "Iter 22/40 - Loss: 10.033 (0.100)\n",
      "Iter 22/40 - Loss: 9.896 (0.100)\n",
      "Iter 22/40 - Loss: 9.987 (0.100)\n",
      "Iter 22/40 - Loss: 9.949 (0.100)\n",
      "Iter 22/40 - Loss: 10.029 (0.100)\n",
      "Iter 22/40 - Loss: 9.974 (0.100)\n",
      "Iter 22/40 - Loss: 9.918 (0.100)\n",
      "Iter 22/40 - Loss: 9.944 (0.100)\n",
      "Iter 23/40 - Loss: 10.081 (0.100)\n",
      "Iter 23/40 - Loss: 9.830 (0.100)\n",
      "Iter 23/40 - Loss: 9.876 (0.100)\n",
      "Iter 23/40 - Loss: 9.897 (0.100)\n",
      "Iter 23/40 - Loss: 9.928 (0.100)\n",
      "Iter 23/40 - Loss: 9.993 (0.100)\n",
      "Iter 23/40 - Loss: 9.963 (0.100)\n",
      "Iter 23/40 - Loss: 9.933 (0.100)\n",
      "Iter 23/40 - Loss: 9.902 (0.100)\n",
      "Iter 23/40 - Loss: 9.746 (0.100)\n",
      "Iter 23/40 - Loss: 9.964 (0.100)\n",
      "Iter 23/40 - Loss: 9.995 (0.100)\n",
      "Iter 23/40 - Loss: 10.099 (0.100)\n",
      "Iter 23/40 - Loss: 10.050 (0.100)\n",
      "Iter 23/40 - Loss: 9.944 (0.100)\n",
      "Iter 23/40 - Loss: 10.340 (0.100)\n",
      "Iter 24/40 - Loss: 10.040 (0.100)\n",
      "Iter 24/40 - Loss: 9.951 (0.100)\n",
      "Iter 24/40 - Loss: 9.996 (0.100)\n",
      "Iter 24/40 - Loss: 9.883 (0.100)\n",
      "Iter 24/40 - Loss: 9.948 (0.100)\n",
      "Iter 24/40 - Loss: 9.887 (0.100)\n",
      "Iter 24/40 - Loss: 9.984 (0.100)\n",
      "Iter 24/40 - Loss: 10.090 (0.100)\n",
      "Iter 24/40 - Loss: 9.812 (0.100)\n",
      "Iter 24/40 - Loss: 9.945 (0.100)\n",
      "Iter 24/40 - Loss: 10.191 (0.100)\n",
      "Iter 24/40 - Loss: 9.926 (0.100)\n",
      "Iter 24/40 - Loss: 9.879 (0.100)\n",
      "Iter 24/40 - Loss: 9.927 (0.100)\n",
      "Iter 24/40 - Loss: 10.009 (0.100)\n",
      "Iter 24/40 - Loss: 9.926 (0.100)\n",
      "Iter 25/40 - Loss: 9.885 (0.100)\n",
      "Iter 25/40 - Loss: 9.868 (0.100)\n",
      "Iter 25/40 - Loss: 10.030 (0.100)\n",
      "Iter 25/40 - Loss: 10.003 (0.100)\n",
      "Iter 25/40 - Loss: 9.970 (0.100)\n",
      "Iter 25/40 - Loss: 9.987 (0.100)\n",
      "Iter 25/40 - Loss: 10.166 (0.100)\n",
      "Iter 25/40 - Loss: 9.904 (0.100)\n",
      "Iter 25/40 - Loss: 9.882 (0.100)\n",
      "Iter 25/40 - Loss: 9.822 (0.100)\n",
      "Iter 25/40 - Loss: 10.114 (0.100)\n",
      "Iter 25/40 - Loss: 9.882 (0.100)\n",
      "Iter 25/40 - Loss: 10.012 (0.100)\n",
      "Iter 25/40 - Loss: 9.948 (0.100)\n",
      "Iter 25/40 - Loss: 10.041 (0.100)\n",
      "Iter 25/40 - Loss: 9.874 (0.100)\n",
      "Iter 26/40 - Loss: 9.941 (0.100)\n",
      "Iter 26/40 - Loss: 9.896 (0.100)\n",
      "Iter 26/40 - Loss: 9.899 (0.100)\n",
      "Iter 26/40 - Loss: 9.804 (0.100)\n",
      "Iter 26/40 - Loss: 10.046 (0.100)\n",
      "Iter 26/40 - Loss: 10.053 (0.100)\n",
      "Iter 26/40 - Loss: 10.133 (0.100)\n",
      "Iter 26/40 - Loss: 9.901 (0.100)\n",
      "Iter 26/40 - Loss: 9.865 (0.100)\n",
      "Iter 26/40 - Loss: 9.916 (0.100)\n",
      "Iter 26/40 - Loss: 9.998 (0.100)\n",
      "Iter 26/40 - Loss: 10.100 (0.100)\n",
      "Iter 26/40 - Loss: 9.904 (0.100)\n",
      "Iter 26/40 - Loss: 10.023 (0.100)\n",
      "Iter 26/40 - Loss: 9.969 (0.100)\n",
      "Iter 26/40 - Loss: 9.919 (0.100)\n",
      "Iter 27/40 - Loss: 10.010 (0.100)\n",
      "Iter 27/40 - Loss: 9.824 (0.100)\n",
      "Iter 27/40 - Loss: 9.959 (0.100)\n",
      "Iter 27/40 - Loss: 9.880 (0.100)\n",
      "Iter 27/40 - Loss: 9.925 (0.100)\n",
      "Iter 27/40 - Loss: 9.930 (0.100)\n",
      "Iter 27/40 - Loss: 10.109 (0.100)\n",
      "Iter 27/40 - Loss: 9.866 (0.100)\n",
      "Iter 27/40 - Loss: 9.846 (0.100)\n",
      "Iter 27/40 - Loss: 9.947 (0.100)\n",
      "Iter 27/40 - Loss: 10.034 (0.100)\n",
      "Iter 27/40 - Loss: 9.931 (0.100)\n",
      "Iter 27/40 - Loss: 10.079 (0.100)\n",
      "Iter 27/40 - Loss: 9.926 (0.100)\n",
      "Iter 27/40 - Loss: 10.200 (0.100)\n",
      "Iter 27/40 - Loss: 9.959 (0.100)\n",
      "Iter 28/40 - Loss: 9.928 (0.100)\n",
      "Iter 28/40 - Loss: 9.997 (0.100)\n",
      "Iter 28/40 - Loss: 9.945 (0.100)\n",
      "Iter 28/40 - Loss: 10.021 (0.100)\n",
      "Iter 28/40 - Loss: 9.944 (0.100)\n",
      "Iter 28/40 - Loss: 10.009 (0.100)\n",
      "Iter 28/40 - Loss: 10.102 (0.100)\n",
      "Iter 28/40 - Loss: 9.986 (0.100)\n",
      "Iter 28/40 - Loss: 10.103 (0.100)\n",
      "Iter 28/40 - Loss: 9.926 (0.100)\n",
      "Iter 28/40 - Loss: 9.934 (0.100)\n",
      "Iter 28/40 - Loss: 9.883 (0.100)\n",
      "Iter 28/40 - Loss: 10.007 (0.100)\n",
      "Iter 28/40 - Loss: 9.807 (0.100)\n",
      "Iter 28/40 - Loss: 9.882 (0.100)\n",
      "Iter 28/40 - Loss: 9.937 (0.100)\n",
      "Iter 29/40 - Loss: 9.920 (0.100)\n",
      "Iter 29/40 - Loss: 9.932 (0.100)\n",
      "Iter 29/40 - Loss: 9.956 (0.100)\n",
      "Iter 29/40 - Loss: 9.938 (0.100)\n",
      "Iter 29/40 - Loss: 10.128 (0.100)\n",
      "Iter 29/40 - Loss: 9.971 (0.100)\n",
      "Iter 29/40 - Loss: 9.960 (0.100)\n",
      "Iter 29/40 - Loss: 10.029 (0.100)\n",
      "Iter 29/40 - Loss: 9.865 (0.100)\n",
      "Iter 29/40 - Loss: 10.020 (0.100)\n",
      "Iter 29/40 - Loss: 10.031 (0.100)\n",
      "Iter 29/40 - Loss: 9.987 (0.100)\n",
      "Iter 29/40 - Loss: 10.008 (0.100)\n",
      "Iter 29/40 - Loss: 10.006 (0.100)\n",
      "Iter 29/40 - Loss: 9.884 (0.100)\n",
      "Iter 29/40 - Loss: 9.742 (0.100)\n",
      "Iter 30/40 - Loss: 10.017 (0.100)\n",
      "Iter 30/40 - Loss: 9.938 (0.100)\n",
      "Iter 30/40 - Loss: 9.921 (0.100)\n",
      "Iter 30/40 - Loss: 10.068 (0.100)\n",
      "Iter 30/40 - Loss: 9.975 (0.100)\n",
      "Iter 30/40 - Loss: 9.936 (0.100)\n",
      "Iter 30/40 - Loss: 10.101 (0.100)\n",
      "Iter 30/40 - Loss: 9.942 (0.100)\n",
      "Iter 30/40 - Loss: 9.971 (0.100)\n",
      "Iter 30/40 - Loss: 9.752 (0.100)\n",
      "Iter 30/40 - Loss: 10.011 (0.100)\n",
      "Iter 30/40 - Loss: 9.949 (0.100)\n",
      "Iter 30/40 - Loss: 9.937 (0.100)\n",
      "Iter 30/40 - Loss: 10.092 (0.100)\n",
      "Iter 30/40 - Loss: 9.916 (0.100)\n",
      "Iter 30/40 - Loss: 9.896 (0.100)\n",
      "Iter 31/40 - Loss: 10.020 (0.010)\n",
      "Iter 31/40 - Loss: 9.892 (0.010)\n",
      "Iter 31/40 - Loss: 9.945 (0.010)\n",
      "Iter 31/40 - Loss: 9.803 (0.010)\n",
      "Iter 31/40 - Loss: 10.016 (0.010)\n",
      "Iter 31/40 - Loss: 9.765 (0.010)\n",
      "Iter 31/40 - Loss: 10.004 (0.010)\n",
      "Iter 31/40 - Loss: 9.985 (0.010)\n",
      "Iter 31/40 - Loss: 10.101 (0.010)\n",
      "Iter 31/40 - Loss: 9.991 (0.010)\n",
      "Iter 31/40 - Loss: 9.837 (0.010)\n",
      "Iter 31/40 - Loss: 9.912 (0.010)\n",
      "Iter 31/40 - Loss: 10.098 (0.010)\n",
      "Iter 31/40 - Loss: 9.773 (0.010)\n",
      "Iter 31/40 - Loss: 10.096 (0.010)\n",
      "Iter 31/40 - Loss: 10.087 (0.010)\n",
      "Iter 32/40 - Loss: 10.005 (0.010)\n",
      "Iter 32/40 - Loss: 10.008 (0.010)\n",
      "Iter 32/40 - Loss: 9.937 (0.010)\n",
      "Iter 32/40 - Loss: 10.086 (0.010)\n",
      "Iter 32/40 - Loss: 9.831 (0.010)\n",
      "Iter 32/40 - Loss: 9.890 (0.010)\n",
      "Iter 32/40 - Loss: 10.089 (0.010)\n",
      "Iter 32/40 - Loss: 9.872 (0.010)\n",
      "Iter 32/40 - Loss: 9.911 (0.010)\n",
      "Iter 32/40 - Loss: 9.931 (0.010)\n",
      "Iter 32/40 - Loss: 9.899 (0.010)\n",
      "Iter 32/40 - Loss: 10.059 (0.010)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 32/40 - Loss: 10.074 (0.010)\n",
      "Iter 32/40 - Loss: 9.914 (0.010)\n",
      "Iter 32/40 - Loss: 9.863 (0.010)\n",
      "Iter 32/40 - Loss: 9.832 (0.010)\n",
      "Iter 33/40 - Loss: 9.965 (0.010)\n",
      "Iter 33/40 - Loss: 9.881 (0.010)\n",
      "Iter 33/40 - Loss: 10.000 (0.010)\n",
      "Iter 33/40 - Loss: 9.966 (0.010)\n",
      "Iter 33/40 - Loss: 9.930 (0.010)\n",
      "Iter 33/40 - Loss: 9.980 (0.010)\n",
      "Iter 33/40 - Loss: 9.923 (0.010)\n",
      "Iter 33/40 - Loss: 9.913 (0.010)\n",
      "Iter 33/40 - Loss: 10.001 (0.010)\n",
      "Iter 33/40 - Loss: 9.969 (0.010)\n",
      "Iter 33/40 - Loss: 9.903 (0.010)\n",
      "Iter 33/40 - Loss: 9.940 (0.010)\n",
      "Iter 33/40 - Loss: 9.946 (0.010)\n",
      "Iter 33/40 - Loss: 10.089 (0.010)\n",
      "Iter 33/40 - Loss: 9.958 (0.010)\n",
      "Iter 33/40 - Loss: 9.845 (0.010)\n",
      "Iter 34/40 - Loss: 9.892 (0.010)\n",
      "Iter 34/40 - Loss: 9.864 (0.010)\n",
      "Iter 34/40 - Loss: 9.993 (0.010)\n",
      "Iter 34/40 - Loss: 10.106 (0.010)\n",
      "Iter 34/40 - Loss: 9.872 (0.010)\n",
      "Iter 34/40 - Loss: 9.969 (0.010)\n",
      "Iter 34/40 - Loss: 9.892 (0.010)\n",
      "Iter 34/40 - Loss: 10.018 (0.010)\n",
      "Iter 34/40 - Loss: 9.852 (0.010)\n",
      "Iter 34/40 - Loss: 9.829 (0.010)\n",
      "Iter 34/40 - Loss: 9.919 (0.010)\n",
      "Iter 34/40 - Loss: 10.116 (0.010)\n",
      "Iter 34/40 - Loss: 9.864 (0.010)\n",
      "Iter 34/40 - Loss: 10.048 (0.010)\n",
      "Iter 34/40 - Loss: 9.908 (0.010)\n",
      "Iter 34/40 - Loss: 10.227 (0.010)\n",
      "Iter 35/40 - Loss: 9.875 (0.010)\n",
      "Iter 35/40 - Loss: 9.864 (0.010)\n",
      "Iter 35/40 - Loss: 10.104 (0.010)\n",
      "Iter 35/40 - Loss: 9.892 (0.010)\n",
      "Iter 35/40 - Loss: 9.805 (0.010)\n",
      "Iter 35/40 - Loss: 9.923 (0.010)\n",
      "Iter 35/40 - Loss: 10.010 (0.010)\n",
      "Iter 35/40 - Loss: 9.915 (0.010)\n",
      "Iter 35/40 - Loss: 10.076 (0.010)\n",
      "Iter 35/40 - Loss: 10.113 (0.010)\n",
      "Iter 35/40 - Loss: 9.884 (0.010)\n",
      "Iter 35/40 - Loss: 10.066 (0.010)\n",
      "Iter 35/40 - Loss: 9.987 (0.010)\n",
      "Iter 35/40 - Loss: 9.910 (0.010)\n",
      "Iter 35/40 - Loss: 9.987 (0.010)\n",
      "Iter 35/40 - Loss: 9.746 (0.010)\n",
      "Iter 36/40 - Loss: 9.946 (0.010)\n",
      "Iter 36/40 - Loss: 10.030 (0.010)\n",
      "Iter 36/40 - Loss: 9.903 (0.010)\n",
      "Iter 36/40 - Loss: 9.855 (0.010)\n",
      "Iter 36/40 - Loss: 10.056 (0.010)\n",
      "Iter 36/40 - Loss: 9.959 (0.010)\n",
      "Iter 36/40 - Loss: 9.948 (0.010)\n",
      "Iter 36/40 - Loss: 9.884 (0.010)\n",
      "Iter 36/40 - Loss: 10.047 (0.010)\n",
      "Iter 36/40 - Loss: 9.885 (0.010)\n",
      "Iter 36/40 - Loss: 10.025 (0.010)\n",
      "Iter 36/40 - Loss: 9.824 (0.010)\n",
      "Iter 36/40 - Loss: 9.862 (0.010)\n",
      "Iter 36/40 - Loss: 9.983 (0.010)\n",
      "Iter 36/40 - Loss: 10.019 (0.010)\n",
      "Iter 36/40 - Loss: 10.054 (0.010)\n",
      "Iter 37/40 - Loss: 10.011 (0.010)\n",
      "Iter 37/40 - Loss: 9.878 (0.010)\n",
      "Iter 37/40 - Loss: 9.980 (0.010)\n",
      "Iter 37/40 - Loss: 9.855 (0.010)\n",
      "Iter 37/40 - Loss: 9.799 (0.010)\n",
      "Iter 37/40 - Loss: 10.032 (0.010)\n",
      "Iter 37/40 - Loss: 9.979 (0.010)\n",
      "Iter 37/40 - Loss: 9.923 (0.010)\n",
      "Iter 37/40 - Loss: 9.926 (0.010)\n",
      "Iter 37/40 - Loss: 10.048 (0.010)\n",
      "Iter 37/40 - Loss: 10.010 (0.010)\n",
      "Iter 37/40 - Loss: 9.890 (0.010)\n",
      "Iter 37/40 - Loss: 9.951 (0.010)\n",
      "Iter 37/40 - Loss: 9.982 (0.010)\n",
      "Iter 37/40 - Loss: 10.075 (0.010)\n",
      "Iter 37/40 - Loss: 9.885 (0.010)\n",
      "Iter 38/40 - Loss: 10.116 (0.010)\n",
      "Iter 38/40 - Loss: 9.992 (0.010)\n",
      "Iter 38/40 - Loss: 9.896 (0.010)\n",
      "Iter 38/40 - Loss: 10.009 (0.010)\n",
      "Iter 38/40 - Loss: 9.855 (0.010)\n",
      "Iter 38/40 - Loss: 9.766 (0.010)\n",
      "Iter 38/40 - Loss: 10.064 (0.010)\n",
      "Iter 38/40 - Loss: 9.869 (0.010)\n",
      "Iter 38/40 - Loss: 9.846 (0.010)\n",
      "Iter 38/40 - Loss: 9.948 (0.010)\n",
      "Iter 38/40 - Loss: 10.119 (0.010)\n",
      "Iter 38/40 - Loss: 9.883 (0.010)\n",
      "Iter 38/40 - Loss: 9.886 (0.010)\n",
      "Iter 38/40 - Loss: 10.032 (0.010)\n",
      "Iter 38/40 - Loss: 9.945 (0.010)\n",
      "Iter 38/40 - Loss: 10.068 (0.010)\n",
      "Iter 39/40 - Loss: 9.899 (0.010)\n",
      "Iter 39/40 - Loss: 9.952 (0.010)\n",
      "Iter 39/40 - Loss: 9.994 (0.010)\n",
      "Iter 39/40 - Loss: 10.064 (0.010)\n",
      "Iter 39/40 - Loss: 10.051 (0.010)\n",
      "Iter 39/40 - Loss: 10.006 (0.010)\n",
      "Iter 39/40 - Loss: 9.938 (0.010)\n",
      "Iter 39/40 - Loss: 9.995 (0.010)\n",
      "Iter 39/40 - Loss: 10.011 (0.010)\n",
      "Iter 39/40 - Loss: 9.810 (0.010)\n",
      "Iter 39/40 - Loss: 9.842 (0.010)\n",
      "Iter 39/40 - Loss: 9.911 (0.010)\n",
      "Iter 39/40 - Loss: 10.017 (0.010)\n",
      "Iter 39/40 - Loss: 9.888 (0.010)\n",
      "Iter 39/40 - Loss: 9.972 (0.010)\n",
      "Iter 39/40 - Loss: 9.860 (0.010)\n",
      "Iter 40/40 - Loss: 10.110 (0.010)\n",
      "Iter 40/40 - Loss: 9.833 (0.010)\n",
      "Iter 40/40 - Loss: 9.916 (0.010)\n",
      "Iter 40/40 - Loss: 9.850 (0.010)\n",
      "Iter 40/40 - Loss: 9.778 (0.010)\n",
      "Iter 40/40 - Loss: 9.903 (0.010)\n",
      "Iter 40/40 - Loss: 10.143 (0.010)\n",
      "Iter 40/40 - Loss: 10.099 (0.010)\n",
      "Iter 40/40 - Loss: 9.916 (0.010)\n",
      "Iter 40/40 - Loss: 9.955 (0.010)\n",
      "Iter 40/40 - Loss: 9.948 (0.010)\n",
      "Iter 40/40 - Loss: 9.954 (0.010)\n",
      "Iter 40/40 - Loss: 9.967 (0.010)\n",
      "Iter 40/40 - Loss: 9.953 (0.010)\n",
      "Iter 40/40 - Loss: 10.014 (0.010)\n",
      "Iter 40/40 - Loss: 9.873 (0.010)\n",
      "CPU times: user 40.9 s, sys: 234 ms, total: 41.2 s\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# We'll do 40 iterations of optimization\n",
    "n_iter = 40\n",
    "\n",
    "# We use SGD here, rather than Adam\n",
    "# Emperically, we find that SGD is better for variational regression\n",
    "optimizer = torch.optim.SGD([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# We use a Learning rate scheduler from PyTorch to lower the learning rate during optimization\n",
    "# We're going to drop the learning rate by 1/10 after 3/4 of training\n",
    "# This helps the model converge to a minimum\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.75 * n_iter], gamma=0.1)\n",
    "\n",
    "# Our loss object\n",
    "# We're using the VariationalMarginalLogLikelihood object\n",
    "mll = gpytorch.mlls.VariationalMarginalLogLikelihood(likelihood, model, n_data=train_y.size(0))\n",
    "\n",
    "# The training loop\n",
    "def train():\n",
    "    for i in range(n_iter):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = torch.autograd.Variable(x_batch.float())\n",
    "            y_batch = torch.autograd.Variable(y_batch.float())\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # We're going to use two context managers here\n",
    "            \n",
    "            # The use_toeplitz flag makes learning faster on the GPU\n",
    "            # See the DKL-MNIST notebook for an explanation\n",
    "            \n",
    "            # The diagonal_correction flag improves the approximations we're making for variational inference\n",
    "            # It makes running time a bit slower, but improves the optimization and predictions\n",
    "            with gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "                output = model(x_batch)\n",
    "                loss = -mll(output, y_batch)\n",
    "                print('Iter %d/%d - Loss: %.3f (%.3f)' % (i + 1, n_iter, loss.data[0], optimizer.param_groups[0]['lr']))\n",
    "            \n",
    "            # The actual optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       2.3790)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADGCAYAAAAwqi48AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCg3KMCgUCg3KMCgUCg3KMCgUCg3/H1x8xeIq9HJxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff93c0ad0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "test_x = torch.autograd.Variable(torch.linspace(0, 1, 51))\n",
    "test_y = torch.sin(test_x * (4 * math.pi))\n",
    "with gpytorch.settings.max_cg_iterations(2000), gpytorch.settings.use_toeplitz(False), gpytorch.beta_features.diagonal_correction():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "lower, upper = observed_pred.confidence_region()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.plot(test_x.cpu().numpy(), test_y.cpu().numpy(), 'k*')\n",
    "ax.plot(test_x.data.cpu().numpy(), observed_pred.mean().data.cpu().numpy(), 'b')\n",
    "ax.fill_between(test_x.data.cpu().numpy(), lower.data.cpu().numpy(), upper.data.cpu().numpy(), alpha=0.5)\n",
    "ax.set_ylim([-3, 3])\n",
    "ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n",
    "print(torch.abs(observed_pred.mean() - test_y)[1:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2601,  0.2821,  0.4847,  0.6909,  0.8480,  0.9338,  0.9695,\n",
       "         0.9532,  0.8785,  0.7430,  0.5709,  0.3812,  0.1573, -0.0799,\n",
       "        -0.3038, -0.5374, -0.7552, -0.9069, -0.9795, -0.9915, -0.9457,\n",
       "        -0.8344, -0.6578, -0.4494, -0.2281,  0.0287,  0.2817,  0.4904,\n",
       "         0.6704,  0.8170,  0.9149,  0.9687,  0.9760,  0.9242,  0.7972,\n",
       "         0.6076,  0.3895,  0.1538, -0.1166, -0.3782, -0.5973, -0.7988,\n",
       "        -0.9568, -1.0364, -1.0353, -0.9724, -0.8643, -0.6859, -0.4613,\n",
       "        -0.2584, -0.0987])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.2487,  0.4818,  0.6845,  0.8443,  0.9511,  0.9980,\n",
       "         0.9823,  0.9048,  0.7705,  0.5878,  0.3681,  0.1253, -0.1253,\n",
       "        -0.3681, -0.5878, -0.7705, -0.9048, -0.9823, -0.9980, -0.9511,\n",
       "        -0.8443, -0.6845, -0.4818, -0.2487,  0.0000,  0.2487,  0.4818,\n",
       "         0.6845,  0.8443,  0.9511,  0.9980,  0.9823,  0.9048,  0.7705,\n",
       "         0.5878,  0.3681,  0.1253, -0.1253, -0.3681, -0.5878, -0.7705,\n",
       "        -0.9048, -0.9823, -0.9980, -0.9511, -0.8443, -0.6845, -0.4818,\n",
       "        -0.2487,  0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2601,  0.0334,  0.0029,  0.0063,  0.0037,  0.0173,  0.0285,\n",
       "         0.0290,  0.0263,  0.0275,  0.0169,  0.0131,  0.0319,  0.0454,\n",
       "         0.0643,  0.0504,  0.0153,  0.0020,  0.0028,  0.0066,  0.0054,\n",
       "         0.0099,  0.0267,  0.0323,  0.0206,  0.0287,  0.0330,  0.0087,\n",
       "         0.0141,  0.0273,  0.0361,  0.0293,  0.0063,  0.0194,  0.0267,\n",
       "         0.0199,  0.0214,  0.0285,  0.0088,  0.0101,  0.0095,  0.0283,\n",
       "         0.0519,  0.0541,  0.0372,  0.0213,  0.0199,  0.0013,  0.0205,\n",
       "         0.0097,  0.0987])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(observed_pred.mean() - test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0200,  0.0400,  0.0600,  0.0800,  0.1000,  0.1200,\n",
       "         0.1400,  0.1600,  0.1800,  0.2000,  0.2200,  0.2400,  0.2600,\n",
       "         0.2800,  0.3000,  0.3200,  0.3400,  0.3600,  0.3800,  0.4000,\n",
       "         0.4200,  0.4400,  0.4600,  0.4800,  0.5000,  0.5200,  0.5400,\n",
       "         0.5600,  0.5800,  0.6000,  0.6200,  0.6400,  0.6600,  0.6800,\n",
       "         0.7000,  0.7200,  0.7400,  0.7600,  0.7800,  0.8000,  0.8200,\n",
       "         0.8400,  0.8600,  0.8800,  0.9000,  0.9200,  0.9400,  0.9600,\n",
       "         0.9800,  1.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
